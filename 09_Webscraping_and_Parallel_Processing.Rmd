---
title: "Webscraping and Parallel Processing"
author:
- affiliation: University of Pennsylvania
  email: gridge@upenn.edu
  name: Greg Ridgeway
- affiliation: University of Pennsylvania
  email: moyruth@upenn.edu
  name: Ruth Moyer
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    css: htmlstyle.css
editor_options: 
  chunk_output_type: console
---

<!-- HTML YAML header Ctrl-Shift-C to comment/uncomment -->

<!-- --- -->
<!-- title: "Webscraping and Parallel Processing" -->
<!-- author: -->
<!-- - Greg Ridgeway (gridge@upenn.edu) -->
<!-- - Ruth Moyer (moyruth@upenn.edu) -->
<!-- date: "`r format(Sys.time(), '%B %d, %Y')`" -->
<!-- output: -->
<!--   pdf_document: -->
<!--     latex_engine: pdflatex -->
<!--   html_document: default -->
<!-- fontsize: 11pt -->
<!-- fontfamily: mathpazo -->
<!-- --- -->

<!-- PDF YAML header Ctrl-Shift-C to comment/uncomment -->


<!-- Make RMarkdown cache the results -->
```{r echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=TRUE, cache.lazy=FALSE, out.width='100%')
```

<!-- A function for automating the numbering and wording of the exercise questions -->
```{r echo=FALSE}
.counterExercise <- 0
.exerciseQuestions <- NULL
.exNum <- function(.questionText="") 
{
   .counterExercise <<- .counterExercise+1
   .questionText <- gsub("@@", "`", .questionText)
   .exerciseQuestions <<- c(.exerciseQuestions, .questionText)
   return(paste0(.counterExercise,". ",.questionText))
}
.exQ <- function(i)
{
   return( paste0(i,". ",.exerciseQuestions[i]) )
}
```


# Introduction 
At the end of our discussion about regular expressions, we introduced the concept of web scraping. Not all online data is in a tidy, downloadable format such as a .csv or .RData file. Yet, patterns in the underlying HTML code and regular expressions together provide a valuable way to "scrape" data off of a webpage. Here, we're going to work through an example of webscraping. We're going to get data on ticket sales of every movie, for every day going back to 2010. 

As a preliminary matter, some R packages, such as `rvest`, can help with web scraping. Eventually you may wish to explore webscraping packages. For now, we are going to work with basic fundamentals so that you have the most flexibility to extract data from most websites.

First, you will need to make sure that you can access the underlying HTML code for the webpage that you want to scrape. If you're using Firefox, you can simply right click on a webpage and then click "View Page Source."  If you're using Microsoft Edge, you can right click on the webpage, click "View Source" and then look at the "Debugger" tab. In Safari select "Preferences" under the "Safari" tab, select the "Advanced" tab, check "Show Develop menu", and then whenever viewing a page you can right click and select "show page source".

Have a look at the webpage [http://www.the-numbers.com/box-office-chart/daily/2018/07/04](http://www.the-numbers.com/box-office-chart/daily/2018/07/04). This page contains information about the movies that were shown in theaters on July 4, 2018 and the amount of money (in dollars) that each of those movies grossed that day. 

Have a look at the HTML code by looking at the page source for this page using the methods described above. The first 10 lines should look  something like this:

```{r comment="", results='as.is', echo=FALSE} 
a <- scan("http://www.the-numbers.com/box-office-chart/daily/2018/07/04",
          what="",sep="\n")
a <- paste("    ",a)
cat(a[1:10], sep="\n")
```

This is all HTML code to set up the page. If you scroll down a few hundred lines, you will find code that looks like this:

```{r comment="", results='as.is', echo=FALSE}
i <- min(grep("#tab=box-office",a))-4
cat(a[i:(i+16)], sep="\n")
```

Here you can see the data! You can see the movie name, ticket sales, number of theaters, and more. It's all wrapped in a lot of HTML code to make it look pretty on a web page, but for our purposes we just want to pull those numbers out.

`scan()` is a basic R function for reading in text, from the keyboard, from files, from the web, ... however data might arrive. Giving `scan()` a URL causes `scan()` to pull down the HTML code for that page and return it to you. Let's try one page of movie data.

```{r comment="", results='hold'} 
a <- scan("http://www.the-numbers.com/box-office-chart/daily/2018/07/04",
          what="", sep="\n")
# examine the first few lines
a[1:5]
```
`what=""` tells `scan()` to expect plain text and `sep="\n"` tells `scan()` to separate each element when it reaches a line feed character, signaling the end of a line.

Some websites are more complex or use different text encoding. On those websites `scan()` produces unintelligible text. The `GET()` function from the `httr` package can sometimes resolve this.
```{r comment="", results='hold'}
library(httr)
resp <- GET("http://www.the-numbers.com/box-office-chart/daily/2018/07/04")
a1 <- content(resp, as="text")
a1 <- strsplit(a1,"\n")[[1]]
cat(a1[1:10], sep="\n")
```

Also, some Mac users will encounter snags with both of these methods and receive "403 Forbidden" errors while their Mac colleague right next to them on the same network will not. I have not figured out why this happens, but have found that making R masquerade as different browser sometimes works.

```{r comment="",results='hold'}
resp <- GET("http://www.the-numbers.com/box-office-chart/daily/2018/07/04", 
            user_agent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.13+ (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2"))
a1 <- content(resp, as="text")
a1 <- strsplit(a1,"\n")[[1]]
```

# Scraping one page

Now that we have stored in the variable `a` the HTML code for one day's movie data in R, let's apply some regular expressions to extract the data. The HTML code includes a lot of lines that do not involve data that interests us. There is code for making the page look nice and code for presenting advertisements. Let's start by finding the lines that have the movie names in them.

Going back to the HTML code, I noticed that both the line with _Jurassic Park_ and _Incredible 2_ have the sequence of characters "#tab=box-office". By finding a pattern of characters that always precedes the text that interests us, we can use it to grep the lines we want. Let's find every line that has "#tab=box-office" in it.

```{r comment="", results='hold'}
i <- grep("#tab=box-office", a)
i
```
These are the line numbers that, if the pattern holds, contain our movie titles. Note that the source code that you might see in your browser may be a little different from the line numbers you see here. Even if you run this code on a different day, you might get different line numbers because some of the code, code for advertisements in particular, can frequently change.

Let's see what these lines of HTML code look like.
```{r comment="", results='hold'}
a[i]
```

Double checking and indeed the first line here is _Jurassic Park: Fallen Kingdom_ and the last line is _Chappaquiddick_. This matches what is on the web page. We now are quite close to having a list of movies that played in theaters on July 4, 2018. However, as you can see, we have a lot of excess symbols and HTML code to eliminate before we can have a neat list of movie names.

HTML tags are always have the form `<some code here>`. Therefore, any text between a less than and greater than symbol we should remove. Here's a regular expression that will look for a `<` followed by a bunch of characters that are not `>` followed by the HTML tag ending `>`... and `gsub()` will delete them.

```{r comment="", results='hold'}
gsub("<[^>]*>", "", a[i])
```

Perfect! Now we just have movie names. You'll see some movie names have strange symbols, like `&hellip;`. That's the HTML code for horizontal ellipses or "...". These make the text look prettier on a webpage, but you might need to do more work with `gsub()` if it is important that these movie names look right. Some movies, like _Ocean's 8_ and _Won't You Be My Neighbor_, have strange text in place of the apostrophe. This is because `scan()` has made some assumptions about the character set used for reading the text on the page... and it turns out that it is not quite right for some special symbols like smart quotes. If we add the parameter `fileEncoding="UTF-8"`, then R will know to interpret the text it is reading using UTF8, a more complete set of characters that includes newer symbols like the euro sign, nicer looking characters like smart quotes, and many non-Latin alphabets.

Let's put these movie names in a data frame, `data0`. This data frame currently has only one column. 

```{r comment="", results='hold'}
data0 <- data.frame(movie=gsub("<[^>]*>", "", a[i]))
```

Now we also want to get the daily gross for each movie. Let's take another look at the HTML code for  _Jurassic Park_.
```{r comment="", results='hold'}
a[i[1]:(i[1]+8)]
```
Note that the movie gross is two lines after the movie name. It turns out that this is consistent for all movies. Since `i` has the line numbers for the movie names, then `i+2` must be the line numbers containing the daily gross.
```{r comment="", results='hold'}
a[i+2]
```
Again we need to strip out the HTML tags. We will also remove the dollar signs and commas so that R will recognize it as a number. We'll add this to `data0` also.
```{r comment="", results='hold'}
data0$gross <- as.numeric(gsub("<[^>]*>|[$,]", "", a[i+2]))
```
Take a look at the webpage and compare it to the dataset you've now created. All the values should now match.
```{r comment="", results='hold'}
head(data0)
tail(data0)
```

# Scraping Multiple Pages

We've now successfully scraped data for one day. This is usually the hardest part. But if we have R code that can correctly scrape one day's worth of data _and_ the website is consistent across days, then it is simple to adapt our code to work for _all_ days. So let's get all movie data from January 1, 2010 through December 31, 2018. That means we're going to be web scraping over 3,200 pages of data.

First note that the URL for July 4, 2018 was 

`http://www.the-numbers.com/box-office-chart/daily/2018/07/04`

We can extract data from any other date by using the same URL, but changing the ending to match the date that we want. Importantly, note that the 07 and the 04 in the URL must have the leading 0 for the URL to return the correct page.

To start, let's make a list of all the dates that we intend to scrape.
```{r comment="", results='hold'}
library(lubridate)
# create a sequence of all days to scrape
dates2scrape <- seq(ymd("2010-01-01"), ymd("2018-12-31"), by="days")
```

Now `dates2scrape` contains a collection of all the dates with movie data that we wish to scrape. 
```{r comment="", results='hold'}
dates2scrape[1:5]
# gsub() can change the - to / to match the appearance of the numbers.com URL
gsub("-", "/", dates2scrape[1:5])
```

Our plan is to construct a for loop within which we will construct a URL from `dates2scrape`, pull down the HTML code from that URL, scrape the movie data into a data frame, and then combine the each day's data frame into one data frame will all of the movie data. First we create a list that will contain each day's data frame.
```{r comment="", results='hold'}
results <- vector("list", length(dates2scrape))
```
On iteration `i` of our for loop we will store that day's movie data frame in `results[[i]]`. The following for loop can take several minutes to run and its speed will depend on your network connection and how responsive the web site is. Before running the entire for loop, it may be a good idea to temporarily set the dates to a short period of time (e.g., a month or two) just to verify that your code is functioning properly. Once you've concluded that the code is doing what you want it to do, you can set the dates so that the for loop runs for the entire analysis period.  

```{r comment="", results='hold'}
timeStart <- Sys.time() # record the starting time
for(iDate in 1:length(dates2scrape))
{
   # uncomment the next line to display progress
   #    useful to know how much is done/left to go
   # print(dates2scrape[iDate])

   # construct URL
   urlText <- paste0("http://www.the-numbers.com/box-office-chart/daily/",
                     gsub("-", "/", dates2scrape[iDate]))

   # read in the HTML code... now using UTF8
   a <- scan(urlText, what="", sep="\n", fileEncoding="UTF-8")

   # find movies
   i <- grep("#tab=box-office", a)
   
   # get movie names and gross
   data0 <- data.frame(movie=gsub("<[^>]*>", "", a[i]),
                       gross=as.numeric(gsub("<[^>]*>|[$,]","",a[i+2])))

   # add date into the dataset
   data0$date  <- dates2scrape[iDate]
    
   results[[iDate]] <- data0
}
# calculate how long it took
timeEnd <- Sys.time()
timeEnd-timeStart
```

Let's look at the first 3 lines of the first and last 3 days.
```{r comment="", results='hold'}
lapply(head(results,n=3), head, n=3)
lapply(tail(results,n=3), head, n=3)
```

Looks like we got them all. Now let's combine them into one big data frame. This use of `do.call()` is a short hand way of saying `rbind(results[[1]], results[[2]], ...)`.
```{r comment="", results='hold'}
movieData <- do.call(rbind, results)

# check that the number of rows and dates seem reasonable
nrow(movieData)
range(movieData$date)
```

If you ran that for loop to gather nearly a decade's worth of data, most likely you walked away from your computer to do something more interesting than watch it print out dates. In these situations, I like to send myself a message when it is complete. The `mailR` package is a convenient way to send yourself an email. If you fill it in with your email, username, and password, the following code will send you an email when the script reaches this point.
```{r comment="", results='hold', eval=FALSE}
library(mailR)
send.mail(from = "", #replace with your email address
          to = c(""), #replace with email addresses to send to
          subject = "Movies",
          body = "R has finished downloading all the movie data",
          smtp = list(host.name="smtp.gmail.com",
                      port     =465,
                      user.name="", # add your username
                      passwd   ="", # and password
                      ssl      =TRUE),
          authenticate = TRUE,
          send = TRUE)
```
Note that the password here is in plain text so do not try this on a public computer. R also saves your history so even if it's not on the screen it might be saved somewhere else on the computer.

# Parallel Computing

Since 1965 Moore's Law has predicted the power of computation over time. Moore's Law predicted the doubling of transistors about every two years. Moore's prediction has held true for decades. However, to get that speed the transistors were made smaller and smaller. Moore's Law cannot continue indefinitely. The diameter of a silicon atom is 0.2nm. Transistors today contain less than 70 atoms and some transistor dimensions are between 10nm and 40nm. Since 2012, computing power has not changed greatly signaling that we might be getting close to the end of Moore's Law, at least with silicon-based computing. What has changed is the widespread use of multiprocessor and multicore processors. Rather than having a single processor, a typical laptop might have an 8 or 16 core processor (meaning they have 8 or 16 processors that share some resources like high speed memory).

R can guess how many cores your computer has on hand.
```{r comment="", results='hold'}
library(doParallel)
detectCores()
```

Having access to multiple cores allows you to write scripts that send different tasks to different processors to work on simultaneously. While one processor is busy scraping the data for January 1st, the second can get to work on January 2nd, and another can work on January 3rd. All the processors will be fighting over the one connection you have to the internet, but they can `grep()` and `gsub()` at the same time other processors are working on other dates.

To write a script to work in parallel, you will need the `doParallel` and `foreach` packages. Let's first test whether parallelization actually speed things up. We've made two foreach loops below. In both of them each iteration of the loop does not really do anything except pause for 2 seconds. The first loop, which does not use parallelization, includes 10 iterations and so should take 20 seconds to run. The second foreach loop looks the same, except right before the foreach loop we have told R to make use of two of the computer's processors rather than the default of one processor. This should cause one processor to sleep for 2 seconds 5 times and the other processor to sleep for 2 seconds 5 times. and should take about 10 seconds.

```{r comment=""}
library(foreach)

# should take 10*2=20 seconds
system.time( # time how long this takes
  foreach(i=1:10) %do% # run not in parallel
  {
     Sys.sleep(2)  # wait for 2 seconds
     return(i)
  }
)

# set up R to use 2 processors
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)

# with two processors should take about 10 seconds
system.time( # time how long this takes
  foreach(i=1:10) %dopar% # run in parallel
  {
    Sys.sleep(2)  # wait for 2 seconds
    return(i)
  }
)

stopCluster(cl)
```

Sure enough, the parallel implementation was able to complete 20 seconds worth of sleeping in only 10 seconds. To set up code to run in parallel, the key steps are to set up the cluster of processors using `makeCluster()` and to tell parallel `foreach()` to use that cluster of processors with `registerDoParallel()`. Note that the key difference between the two `foreach()` statements is that the first `foreach()` is followed by a `%do%` while the second is followed by a `%dopar%`. When `foreach()` sees the `%dopar%` it will check what was setup in the `registerDoParallel()` call and spread the computation among those processors.

Note that the `foreach()` differs a little bit in its syntax compared with our previous use of for loops. While for loops have the syntax `for(i in 1:10)` the syntax for `foreach()` looks like `foreach(i=1:10)` and is followed by a `%do%` or a `%dopar%`. Lastly, note that the final step inside the `{ }` following  a `foreach()` is a `return()` statement. `foreach()` will take the returned values of each of the iterations and assemble them into a single list by default. In the following `foreach()` we've added `.combine=rbind` to the `foreach()` so that the final results will be stacked into one data frame, avoiding the need for a separate `do.call()` like we used previously.

With all this in mind, let's web scrape the movie data using multiple processors:

```{r comment="", results='hold'}
cl <- makeCluster(8)
registerDoParallel(cl)

timeStart <- Sys.time() # record the starting time
movieData <-
   foreach(iDate=1:length(dates2scrape),
           .combine=rbind) %dopar%
{
   urlText <- paste0("http://www.the-numbers.com/box-office-chart/daily/",
                     gsub("-", "/", dates2scrape[iDate]))

   a <- scan(urlText, what="", sep="\n", fileEncoding="UTF-8")
   i <- grep("#tab=box-office", a)
   
   data0 <- data.frame(movie=gsub("<[^>]*>", "", a[i]),
                       gross=as.numeric(gsub("<[^>]*>|[$,]","",a[i+2])))

   data0$date  <- dates2scrape[iDate]
    
  return(data0)
}

# change HTML codes to something prettier
movieData$movie <- gsub("&hellip;", "...", movieData$movie)

# calculate how long it took
timeEnd <- Sys.time()
timeEnd-timeStart

stopCluster(cl)
```

This code makes use of 8 processors. Unlike our 2 second sleep example, this script may or may not run 8 times faster. This is mostly due to the fact each processor still needs to wait its turn in order to pull down its webpage from the internet. Still you should observe the parallel version finishing much sooner than the first version. In just a few lines of code and a few minutes of waiting, you now have almost a decade worth of movie data.

Parallelization introduces two complications. The first is that if anything goes wrong in this script, then the whole `foreach()` fails. For example, let's say that after scraping movie data from 2000-2016 you briefly lose your internet connection. If this happens, then `scan()` fails and the whole `foreach()` ends with an error, tossing all of your already complete computation. To avoid this you need to either be sure you have a solid internet connection, or wrap the call to `scan()` in a `try()` and a `while` loop that is smart enough to wait a few seconds and try the scan again rather than fail completely. A second, and minor issue, is that you cannot print to the screen from inside a parallelized `foreach()`. So there's no printing the progress of the computation to the screen. This can leave you wondering if your script is still working. You can set an `outfile` parameter when creating your cluster of processors, like `makecluster(8, outfile="log.txt")`. Any output that R prints will be redirected to this log.txt file. Remember that all the processors will be at different steps and they will all be dumping output to log.txt. It can be difficult to determine which process is responsible for which line of output, but at least you will know that your script is progressing.

It's probably wise at this point to save `movieData` so that you won't have to rerun this in the future.
```{r comment="", results='hold'}
save(movieData, file="movieData.RData", compress=TRUE)
```

# Fun With Movie Data

You can use the dataset to answer questions such as "which movie yielded the largest gross?"
```{r comment="", results='hold'}
movieData[which.max(movieData$gross),]
```
Which ten movies had the largest total gross during the period this dataset covers? 
```{r comment="", results='hold'}
a <- aggregate(gross~movie, data=movieData, sum)
a[order(-a$gross)[1:10],]
```
Which days of the week yielded the largest total gross?
```{r comment="", results='hold'}
aggregate(gross~wday(date, label=TRUE), data=movieData, sum)
```

Now that you have movie data and in a previous section you assembled Chicago crime data, combine the two datasets so that you can answer the question "what happens to crime when big movies come out?"