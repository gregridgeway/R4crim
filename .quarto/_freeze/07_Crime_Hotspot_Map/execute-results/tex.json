{
  "hash": "ef4a39ab7d8bb5ab0b067e167898e89f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Making a Hotspot Map\"\nauthor:\n- affiliation: University of Pennsylvania\n  email: gridge@upenn.edu\n  name: Greg Ridgeway\n- affiliation: University of Pennsylvania\n  email: moyruth@upenn.edu\n  name: Ruth Moyer\n- affiliation: University of Pennsylvania\n  email: gohl@upenn.edu\n  name: Li Sian Goh\ndate: \"August 17, 2025\"\nformat:\n  html:\n    theme: \n      dark: darkly\n      light: default\n    toc: true\n    html-math-method: mathjax\n  pdf:\n    toc: true\n    prefer-html: true\n    fig-format: png\nnumber-sections: true\neditor_options: \n  chunk_output_type: console\nbibliography: G:/My Drive/docs/Greg/articles/mybib.bib\n---\n\n\n\n\n<!-- In terminal -->\n<!-- quarto render 07_Crime_Hotspot_Map.qmd -->\n\n<!-- git commit 07-* -m \"commit message\" -->\n<!-- git status -->\n<!-- git push -->\n\n\n<!-- A function for automating the numbering and wording of the exercise questions -->\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- This document can't be created after running Lesson #8 SQL part 2. To recreate this document, first rerun Lesson #6 SQL Part 1. -->\n\nNote: If you have skipped ahead and run the code in Lession \\#8: SQL Part 2, then some of the queries involving `PrimaryType` in this set of notes will not work. You will need to modify those queries to left join with the `iucr` table.\n\n# Introduction\n\nIn the previous section, you built a SQLite database to store and manage the large Chicago crime data. In this section, you will query the database to extract the latitude and longitude of crime incidents and create a hotspot map with a large dataset. The concept of determining the geographic locations where crime is most intense, a crime hotspot, is very important to criminological theory as well as to the very practical question of where to focus public safety efforts. \n\nHere we will make hotspot maps with our Chicago crime data (which requires use of SQL). Of course, if you have acquired a dataset with latitude and longitude not in a SQL database, you could use these same methods to make a hotspot map using only R.\n\n# Setting up the data\nLet's first load up the `sqldf` library and reconnect to our Chicago crime database.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sqldf)\ncon <- dbConnect(SQLite(), dbname=\"chicagocrime.db\")\n```\n:::\n\n\n\n\nLet's run a SQL query to extract the two columns, `Latitude` and `Longitude`, from our crime database, creating a data frame with one row for each crime incident.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataAllcrime <- dbGetQuery(con, \"\n   SELECT Latitude, Longitude\n   FROM crime\n   WHERE Latitude  IS NOT NULL AND\n         Longitude IS NOT NULL\")\nnrow(dataAllcrime)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8284414\n```\n\n\n:::\n:::\n\n\n\n\nHow would your query differ if you wanted to do a hotspot map of only assaults? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataAssaults <- dbGetQuery(con, \"\n   SELECT Latitude, Longitude\n   FROM crime\n   WHERE PrimaryType='ASSAULT' AND\n         Latitude  IS NOT NULL AND\n         Longitude IS NOT NULL\")\nnrow(dataAssaults)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 557660\n```\n\n\n:::\n:::\n\n\n\nAlways do a quick check to make shre the data looks the way you expect it to look.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataAssaults |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Latitude Longitude\n1 41.91944 -87.77556\n2 41.96846 -87.70761\n3 41.74878 -87.60517\n4 41.76889 -87.61413\n5 41.87296 -87.75366\n6 41.88448 -87.63131\n```\n\n\n:::\n:::\n\n\n\n\n# Creating leaflet maps\n\nWe will be using `leaflet` to make our maps, so be sure you have it installed. We will also use the `sf` package for some of our spatial calculations. Here we avoid saying too much about the `sf` package since we go into great detail about it in later parts of the course.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(sf)\n```\n:::\n\n\n\nLet's start small by creating a leaflet map just showing assaults in Ward 22.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataWard22Assaults <- dbGetQuery(con, \"\n   SELECT Latitude,Longitude\n   FROM crime\n   WHERE PrimaryType='ASSAULT' AND\n         Latitude  IS NOT NULL AND\n         Longitude IS NOT NULL AND\n         Ward=22\")\n\nleaflet(dataWard22Assaults) |>\n  setView(lng=-87.73, lat=41.83, # selected map's center\n          zoom=13) |>            # zoom in to \"neighborhood\" level\n  addTiles() |> # add the base street layer\n  addCircleMarkers(~Longitude, ~Latitude,\n                   radius=3, \n                   stroke=FALSE,\n                   fillOpacity = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nfile:///C:/Users/greg_/AppData/Local/Temp/Rtmp4k8bfh/file7abc9c66bbf/widget7abc31da40a2.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/leafletWard22assaults-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nLeaflet has placed dots all over the map for ward 22. With so many dots, it is rather difficult to determine where exactly there is a higher crime density. It seems that crime is simply everywhere in ward 22. Instead of showing all the dots, we are going to make a new map layer that colors different areas depending on the number of crime incidents per square kilometer, the crime density.\n\n## Hexagonal tiling\n\nOne strategy is to chop up the Chicago map into a bunch of smaller tiles. For this we are going to overlay our leaflet map with hexagonal tiles. Then we will count how many dots land within each hexagonal tile and color the tile according to that count. Hexagonal bins are convenient because they produce smoother, more visually appealing density maps, especially when compared with square tiles. Each hexagon has a consistent distance to its neighbors and their geometry better approximates a circle, reducing distortion in how densities appear. This uniformity makes patterns in the underlying data easier to interpret, especially when mapping phenomena like crime incidents, where you want to highlight true spatial concentrations rather than artifacts of the grid shape.\n\nThis time we will make a hotspot map of all assaults in Chicago. First, we need to communicate to R that latitude and longitude are special geographic coordinates. We will use a special coordinate system, the [Universal Transverse Mercator](https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system) coordinate system, for the part of globe sharing Chicago's longitude area (CRS 26916 = UTM zone 16N) to avoid distortion (because we are projecting points on a round earth to a two dimensional screen). The problem with using latitude and longitude coordinates is that differences in coordinates are measured in degrees rather than meters. At the equator 0.01° longitude is about 1 kilometer, but 0.01° degree gets shorter and shorter as you move toward the earth's poles. Therefore, all of our tiling and counting will be done in the UTM Zone 16N coordinate system that has distances measured in meters. After we have done all of our calculations, we will convert back to latitude and longitude to overlay our results on top of leaflet.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert to an sf object\n#   tells R that Longitude and Latitude are special columns\ndataAssaults <- dataAssaults |>\n   st_as_sf(coords = c(\"Longitude\", \"Latitude\"), \n            crs = 4326,        # lat/long coordinate system (WGS 84)\n            remove = FALSE) |> # don't remove the lat/long columns\n   st_transform(crs = 26916)   # project to UTM 16N, Chicago\n```\n:::\n\n\n\n\nNext, we will create the set of hexagonal tiles that covers the assault locations. We need to set the size of the hexagons, specifically the distance between opposite edges of each hexagon. First check what units of distance R will use with these data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check the distance units dataAssaults is in\nst_crs(dataAssaults)$units\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"m\"\n```\n\n\n:::\n:::\n\n\n\nConfirming that the distance measurements are in meters, let's set the hexagon size to 0.5 km or 500 meters. After that, we can use `st_make_grid()` to make an R object (an `sf` object) that creates the hexagons and stores them in a convenient format.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set distance (meters) between opposite sides of each hexagon\nsizeHex <- 500\n# create hexagonal tiles\ngridHex <- st_make_grid(dataAssaults, \n                        cellsize = sizeHex, \n                        square = FALSE) |> # prefer hexagons\n   st_as_sf() |> # convert to sf object\n   rename(geometry = x)\ngridHex\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 6762 features and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 422095.9 ymin: 4610178 xmax: 456845.9 ymax: 4652757\nProjected CRS: NAD83 / UTM zone 16N\nFirst 10 features:\n                         geometry\n1  POLYGON ((422345.9 4610611,...\n2  POLYGON ((422345.9 4611477,...\n3  POLYGON ((422345.9 4612343,...\n4  POLYGON ((422345.9 4613209,...\n5  POLYGON ((422345.9 4614075,...\n6  POLYGON ((422345.9 4614941,...\n7  POLYGON ((422345.9 4615807,...\n8  POLYGON ((422345.9 4616673,...\n9  POLYGON ((422345.9 4617539,...\n10 POLYGON ((422345.9 4618405,...\n```\n\n\n:::\n:::\n\n\n\n`gridHex` is like a regular data frame with one column containing polygon objects as each row's data element. There is also a header that gives some summary information like how many hexagons there are in `gridHex` (6,762) and its coordinate system (NAD83 / UTM zone 16N).\n\nLet's check our work so far by displaying the hexagons over a leaflet map of Chicago.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleaflet() |>\n   addTiles() |>\n   # zoom=11 -> tight city level view\n   setView(lng = -87.8, lat = 41.85, zoom = 11) |>\n   addPolygons(\n      # convert coordinates back to lat/long (CRS 4326)\n      data = gridHex |> st_transform(crs=4326),\n      fillOpacity = 0,    # don't color in the hexagons\n      color = \"darkgray\", # border of hexagons\n      weight = 2,         # border thickness\n      opacity = 1)        # border transparency\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nfile:///C:/Users/greg_/AppData/Local/Temp/Rtmp4k8bfh/file7abc580f25f8/widget7abc5f26612.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/leafletHexTiles-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nWe are making progress. We have covered the map of Chicago with 6,762 hexagons. Some of our hexagons are tiling parts of Lake Michigan. We will end up dropping those by filtering our hexagons with 0 crime counts.\n\nNow we need to count how many of the assaults land in each of these hexagonal tiles. `st_intersects()` will figure out which points in `dataAssaults` land inside each hexagon and `lengths()` will count their number. We will also compute the crime density (incidents per square kilometer per year), eliminate any hexagons with 0 assaults (like all those in Lake Michigan), and project coordinates back to latitude/longitude, which leaflet requires.\n\nWe will need to know the number of years of data we have so we can normalize results to a \"per year\" rate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- dbGetQuery(con, \"SELECT Date FROM crime\") |>\n   mutate(Date = lubridate::mdy_hms(Date)) |>\n   summarize(firstDate = min(Date),\n             lastDate  = max(Date))\nnYears <- as.numeric(difftime(a$lastDate, a$firstDate, units = \"days\") / 365.25)\nnYears\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 24.59138\n```\n\n\n:::\n:::\n\n\n\n\n`st_intersects()`, when used with polygon shapes (`gridHex`) and points (`dataAssaults`), returns for each polygon a list of the indices of points that the polygon intersects. Here is what the results of `st_intersects()` look like for 10 hexagons plucked from the middle of Chicago. For some hexagons, the list is empty signalling that no assaults landed in that hexagon. For other hexagons we see lists of points. These are the row numbers from `dataAssaults` of points that landed in that specific hexagonal tile.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngridHex |>\n   slice(3480:3489) |>\n   st_intersects(dataAssaults)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSparse geometry binary predicate list of length 10, where the predicate\nwas `intersects'\n 1: (empty)\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: (empty)\n 6: 12, 3388, 6990, 7234, 7427, 7487, 18670, 19382, 20377, 27893, ...\n 7: 9391, 14771, 18938, 19218, 25833, 30536, 31292, 31427, 33952, 43080, ...\n 8: 29548, 30511, 37116, 49594, 54512, 58866, 95299, 121602, 127701, 169652, ...\n 9: 7158, 11117, 17344, 20963, 42193, 92508, 99246, 134087, 141568, 142694, ...\n 10: 146006, 150503, 378307\n```\n\n\n:::\n:::\n\n\n\nIf we apply `lengths()` to this list it will return the number of points in each of these lists.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngridHex |>\n   slice(3480:3489) |>\n   st_intersects(dataAssaults) |>\n   lengths()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  0  0  0  0  0 94 98 34 38  3\n```\n\n\n:::\n:::\n\n\n\n\nNow for all of our hexagons covering Chicago let's compute the number of assaults per km^2^ per year.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngridHex <- gridHex |>\n   # find which dots each hex intersects...\n   mutate(nAssaults = geometry |>\n             st_intersects(dataAssaults) |> \n             lengths(), # ...and count them\n          # get area in square kilometers\n          areakm2 = as.numeric(st_area(geometry)) / 10^6,\n          # compute incidents per km2 per year\n          density = nAssaults / areakm2 / nYears) |>\n   filter(nAssaults > 0) |> # drop those that are empty\n   st_transform(crs=4326) # transform back to lat/long for leaflet\n\ngridHex\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 2667 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.94064 ymin: 41.64168 xmax: -87.51863 ymax: 42.02491\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                         geometry nAssaults   areakm2   density\n1  POLYGON ((-87.93757 41.9944...         1 0.2165064 0.1878220\n2  POLYGON ((-87.93472 42.0061...         1 0.2165064 0.1878220\n3  POLYGON ((-87.93153 41.9944...         1 0.2165064 0.1878220\n4  POLYGON ((-87.92789 41.9516...         1 0.2165064 0.1878220\n5  POLYGON ((-87.928 41.9594, ...         1 0.2165064 0.1878220\n6  POLYGON ((-87.92846 41.9905...         1 0.2165064 0.1878220\n7  POLYGON ((-87.92538 41.9867...         4 0.2165064 0.7512881\n8  POLYGON ((-87.92561 42.0023...         3 0.2165064 0.5634661\n9  POLYGON ((-87.92186 41.9516...         1 0.2165064 0.1878220\n10 POLYGON ((-87.92197 41.9594...         1 0.2165064 0.1878220\n```\n\n\n:::\n:::\n\n\n\nNow `gridHex` has three new columns associating to each hexagon an assault count, the area of the hexagon (all should be identical), and the assault density. We want to break up that density into bins and assign a color to each bin. This way we can color-code each hexagon based on the assault density. `colorBin()` creates a function that will take a density value and return a color. The combination of `bins = 10` and `pretty = TRUE` will create some nice intervals for the different colors. Viridis is a family of color palettes that has been engineered to be perceptually uniform (equal steps in data values look like equal steps in color), colorblind-friendly, printer-friendly (maintains contrast in grayscale), and is readable on screens and projectors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npal <- colorBin(\"viridis\",\n                domain = gridHex$density,\n                bins = 10,\n                pretty = TRUE) # choose \"nice\" breakpoints\n```\n:::\n\n\n\nSo what is this `pal()` function? If we give `pal()` a number it will produce a color code.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npal(125)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"#3B528B\"\n```\n\n\n:::\n:::\n\n\n\n`pal(125)` produces the color code #3B528B, which gives the hexadecimal code for mixing the primary source colors red (3B), green (52), and blue (8B). That mix is like a deep indigo blue. When creating the hexagon overlay, each hexagon's `density` will be run through `pal()` which will determine how to color the hexagon.\n\nWhat breakpoints did `pal()` decide on?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npal |> attr(\"colorArgs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$bins\n [1]   0  50 100 150 200 250 300 350 400 450\n\n$na.color\n[1] \"#808080\"\n```\n\n\n:::\n:::\n\n\n\nThe `bins` value shows that `pal()` will assign one color to 0-50, another color to 50-100, and so on up to the last bin 400-450. It also shows that if there happens to be any hexagons with missing values for `density` (there are in fact none), those hexagons will get colored with \\#808080, which is gray.\n\nLet's make our leaflet map!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleaflet() |>\n  addTiles() |>\n  setView(lng = -87.8, lat = 41.85, zoom = 10) |>\n  addPolygons(\n    data = gridHex,\n    fillColor = ~pal(density),\n    fillOpacity = 0.5,\n    color = \"darkgray\", # border of hexagons\n    weight = 0.5,       # border thickness\n    opacity = 0.8,      # border transparency\n    # make popup box when hovering\n    label = ~paste(\"Count:\",\n                   format(nAssaults, big.mark=\",\"),\n                   \"<br>Density: \",\n                   format(round(density,1), nsmall=1),\n                   \"/ km² / year\") |>\n       lapply(htmltools::HTML), # signal HTML so <br> is linebreak\n    highlightOptions = highlightOptions(color = \"red\", # hover highlight\n                                        weight = 2, \n                                        bringToFront = FALSE)) |>\n  addLegend(position = \"bottomright\",\n            pal = pal, \n            values = gridHex$density,\n            title = \"Incidents / km² / year\",\n            opacity = 0.7, \n            labFormat = labelFormat()) # use pal() to figure out format\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nfile:///C:/Users/greg_/AppData/Local/Temp/Rtmp4k8bfh/file7abc1d7b3012/widget7abc7a686ac8.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/leafletHexMap-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nYou can zoom in on specific neighborhoods or specific hexagons. You can also hover your mouse over a hexagon to see the assault density.\n\nYou can try to make your own color palettes, like\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npalGreg <- colorNumeric(\n  palette = colorRampPalette(c(\"springgreen\", \"hotpink\"))(10),\n  domain = c(0, 450))\n\nleaflet() |>\n  addTiles() |>\n  setView(lng = -87.8, lat = 41.85, zoom = 10) |>\n  addPolygons(\n    data = gridHex,\n    fillColor = ~palGreg(density),\n    fillOpacity = 0.5,\n    color = \"darkgray\",\n    weight = 0.5,\n    opacity = 0.8,\n    label = ~paste(\"Count:\",\n                   format(nAssaults, big.mark=\",\"),\n                   \"<br>Density: \",\n                   format(round(density,1), nsmall=1),\n                   \"/ km² / year\") |>\n       lapply(htmltools::HTML),\n    highlightOptions = highlightOptions(color = \"red\",\n                                        weight = 2, \n                                        bringToFront = FALSE)) |>\n  addLegend(position = \"bottomright\",\n            pal = palGreg, \n            values = gridHex$density,\n            title = \"Incidents / km² / year\",\n            opacity = 0.7, \n            labFormat = labelFormat())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nfile:///C:/Users/greg_/AppData/Local/Temp/Rtmp4k8bfh/file7abc26f64539/widget7abc4f4933ba.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/leafletHexMapGreenPink-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nYikes! I have made awful color choices here. I find it best to rely on the built in color palettes that have been more thoughtfully constructed.\n\n## Contour map\n\nYou can also create a map that shows the high crime areas in a way that is akin to elevation on a topographical map. The more concentrated the concentric areas, the higher the crime (or, in the case of a topographical map, the higher the terrain).\n\n### Kernel density estimation\nWe are going to use a two-dimensional kernel density estimate. Two-dimensional kernel density estimation (2D KDE) is a smoothing technique that transforms a set of crime incident points into a smooth looking surface, highlighting areas where events are more concentrated. Instead of plotting individual dots, KDE places a smooth \"bump\" (the kernel) over each point and sums them up, producing a measure of how many incidents are expected per unit area. When used for crime hotspot mapping, this method makes clusters of incidents visually stand out, reduces noise from random scatter, and allows analysts to interpret patterns of concentration at a chosen spatial scale (controlled by a bandwidth parameter).\n\nTo give the general idea how this works, let's look at a one-dimensional version. Imagine we have single street that is 6 kilometers long and we know the crime locations of 8 crimes along this street. I have marked those 8 crime locations on the horizontal axis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Small set of points (e.g., crime locations on a 1D street axis)\nx <- c(1.0, 1.8, 2.0, 2.7, 3.1, 3.9, 4.2, 5.0)\nplot(0, 0, type = \"n\", \n     xlim=c(0, 6),\n     ylim = c(0, 1.8),\n     xlab = \"x\", ylab = \"\",\n     axes = FALSE)\naxis(1)\nrug(x, lwd = 2)\npoints(x, rep(0, length(x)), pch = 19)\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-16-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nCentered on each point place a bell-shaped curve. I have used the normal distribution here, but other choices are possible. The parameter `bandwidth` controls how wide these bell curves are around each point. The total area under these bell curves adds up to 8, the total number of incidents. Rather than treat the incident as occurring at one specific point, the kernels kind of smudge the incident so that the weight of any one incident is spread over nearby areas.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(0, 0, type = \"n\",\n     xlim=c(0, 6),\n     ylim = c(0, 1.8),\n     xlab = \"x\", ylab = \"Incidents\")\nrug(x, lwd = 2)\npoints(x, rep(0, length(x)), pch = 19)\n\n# Bandwidth (controls smoothness)\nh <- 0.35\n\n# Grid to evaluate densities\ng <- seq(min(x) - 3*h, max(x) + 3*h, length.out = 1000)\n\n# compute kernels\nkernels <- sapply(x, function(xi) dnorm((g - xi)/h) / h)\n\n# Overlay individual kernels (light lines)\napply(kernels, 2, \n      function(y) lines(g, y, \n                        col = rgb(0.2, 0.2, 0.2, 0.35), \n                        lwd = 1.5))\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-17-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nTo estimate the number of incidents we can sum those kernel values at any point $x$. So if you want to compute the KDE of the incidents at 2.5. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(0, 0, type = \"n\", \n     xlim=c(0, 6), ylim = c(0, 1.8),\n     xlab = \"x\", ylab = \"Incidents\")\nrug(x, lwd = 2)\npoints(x, rep(0, length(x)), pch = 19)\napply(kernels, 2, \n      function(y) lines(g, y, \n                        col = rgb(0.2, 0.2, 0.2, 0.35), \n                        lwd = 1.5))\n\n# find the kernel heights near 2.5\ni <- abs(g-2.5) |> which.min()\npoints(rep(2.5, 8), c(kernels[i,1:8]), col=rgb(1,0,0,0.35), pch=19)\npoints(2.5, sum(kernels[i,1:8]), col=\"red\", pch=19)\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-18-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nThat sum is 1.8. A more useful measure is an incident rate per kilometer. Since we have a 6-kilometer street segment, the crime incident rate at the 2.5 km marker divides that sum by 6, 0.30 incidents per kilometer. I will divide the KDE by 6 through the rest of this section so that the vertical axis is incidents / km.\n\nRepeat this process for a range of values of $x$. Note that I am summing up the kernel values and then dividing by 6 (the length of the road) to get an incident per kilometer rate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(0, 0, type = \"n\",\n     xlim=c(0, 6), ylim = c(0, 1.2),\n     xlab = \"x\", ylab = \"Incidents / km\")\nrug(x, lwd = 2)\npoints(x, rep(0, length(x)), pch = 19)\napply(kernels, 2, \n      function(y) lines(g, y, \n                        col = rgb(0.2, 0.2, 0.2, 0.35), \n                        lwd = 1.5))\n\n# KDE = sum of kernels\nkde <- rowSums(kernels) / 6 # divide by number of kms -> incidents/km\nlines(g, kde, col = \"#2C7BB6\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-19-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nSelecting a bandwidth can be a little tricky. A bandwidth that is too small gives a KDE that is unstable and too jagged. A bandwidth that is too large smooths away interesting features and fails to highlight hotspots. The R `MASS` package has useful tools for choosing reasonable bandwidth parameters that we will use on our Chicago data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(0, 0, type = \"n\",\n     xlim=c(0, 6), ylim = c(0, 1.2),\n     xlab = \"x\", ylab = \"Incidents / km\")\nrug(x, lwd = 2)\npoints(x, rep(0, length(x)), pch = 19)\n\n# too small\nh <- 0.1\nkernels <- sapply(x, function(xi) dnorm((g - xi)/h) / h)\nlines(g, rowSums(kernels) / 6, col = \"#E66101\", lwd = 3)\n\n# too big\nh <- 1.0\nkernels <- sapply(x, function(xi) dnorm((g - xi)/h) / h)\nlines(g, rowSums(kernels) / 6, col = \"#4DAF4A\", lwd = 3)\n\n# original h=0.35\nh <- 0.35\nkernels <- sapply(x, function(xi) dnorm((g - xi)/h) / h)\nlines(g, kde, col = \"#2C7BB6\", lwd = 3)\n\nlegend(\"topright\",\n       legend = c(\"h=0.10\", \"h=0.35\", \"h=1.00\"),\n       fill   = c(\"#E66101\",\"#2C7BB6\",\"#4DAF4A\"),\n       border = NA,\n       bty    = \"n\",\n       title  = \"Bandwidth\")\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-20-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nThe color that we would apply to any stretch of the road depends on how high the crime density is at a particular point. I have shaded the regions showing the relationship between the KDE estimate and what color we would shade the road segment.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(0, 0, type = \"n\",\n     xlim=c(0, 6), ylim = c(0, 1.2),\n     xlab = \"x\", ylab = \"\")\n\npal1dKDE <- colorBin(\"viridis\",\n                     domain = kde,\n                     bins = 4,\n                     pretty = TRUE)\nrect(par()$usr[1], 0.3, par()$usr[2], par()$usr[4], \n     col=pal1dKDE(0.35) |> adjustcolor(alpha.f = 0.5))\nrect(par()$usr[1], 0.2, par()$usr[2], 0.3, \n     col=pal1dKDE(0.25) |> adjustcolor(alpha.f = 0.5))\nrect(par()$usr[1], 0.1, par()$usr[2], 0.2, \n     col=pal1dKDE(0.15) |> adjustcolor(alpha.f = 0.5))\nrect(par()$usr[1], 0.0, par()$usr[2], 0.1, \n     col=pal1dKDE(0.05) |> adjustcolor(alpha.f = 0.5))\n\nrug(x, lwd = 2)\npoints(x, rep(0, length(x)), pch = 19)\nlines(g, kde, col = \"#2C7BB6\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-21-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nTo simplify, we draw the road colored by crime incident rate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(g, rep(0,1000), ylim = c(0, 1.2),\n     xlab = \"x\", ylab = \"\",\n     col=pal1dKDE(kde))\nrug(x, lwd = 2)\npoints(x, rep(0, length(x)), pch = 19)\nbins <- attr(pal1dKDE, \"colorArgs\")$bins\nmid  <- (bins[-1] + bins[-length(bins)]) / 2\nbins <- bins |> format(nsmall=1)\nlabs <- paste0(\"(\", bins[-length(bins)], \", \", bins[-1], \")\")\nlegend(\"topright\",\n       legend = labs,\n       fill   = pal1dKDE(mid),\n       border = NA,\n       bty    = \"n\",\n       title  = \"Incidents / km\")\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-22-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nThe process of computing a KDE for a two-dimensional collection of points is similar. We use a two-dimensional version of the normal distribution kernel and create a two-dimensional grid of values at which we will compute the incident rate per square kilometer.\n\n### Contour map with two-dimensional kernel density estimation\nThe `MASS` package provides a two-dimensional KDE function `kde2d()`. We will also use the `isoband` package. Isobands are the filled regions between pairs of contour lines representing areas where the incident rate values fall within a given range.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for computing 2-dimensional kernel density estimates\nlibrary(MASS)\n# for creating polygons from contours\nlibrary(isoband)\n```\n:::\n\n\n\nHere we use `bandwidth.nrd()` to select bandwidths, one for the x direction and one for the y direction. `kde2d()` will compute the KDE on a 200 x 200 grid of points.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the (x,y) coordinates\nxy <- dataAssaults |>\n   st_coordinates() |>\n   data.frame()\n# simple bandwidth guess for how much smoothing\nh <- c(bandwidth.nrd(xy$X), bandwidth.nrd(xy$Y))  \nkdeAssaults <- kde2d(\n   xy$X, xy$Y, # coordinates\n   n = 200,    # a 200 x 200 grid\n   h = h)      # bandwidth\n```\n:::\n\n\n\nStored in `kdeAssaults` is a component `z` that contains the crime density estimate. As we requested, `kde2d()` chopped the map of Chicago into a 200 x 200 grid of dimension and `z` contains the estimated crime density at those grid points. Those points are centered in a box with width and height equal to:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# width in meters\ndiff(kdeAssaults$x[1:2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 169.6576\n```\n\n\n:::\n\n```{.r .cell-code}\n# height in meters\ndiff(kdeAssaults$y[1:2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 211.274\n```\n\n\n:::\n:::\n\n\n\n`z` is scaled so that `z` times the width times the height equals the fraction of expected crime incidents in that box. So `z` is like the fraction of crime incidents per square meter. If we multiply each value of `z` by the width and height of the boxes (which all have the same size) and add them up, we should get a number close to 1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(kdeAssaults$z * diff(kdeAssaults$x[1:2]) * diff(kdeAssaults$y[1:2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9973903\n```\n\n\n:::\n:::\n\n\n\nIf we multiply all the values of `z` by the total number of crime incidents, then rather than describing the fraction of incidents per square meter (a hard to understand quantity), we estimate the expected number of incidents per square meter. Multiply that by 10^6^ to convert square meters to square kilometers and divide by the number of years of data that we have so that we get an estimated number of crime incidents per km^2^ per year, a measurement that describes the pace of incidents over space and time.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# scale so that the units are incidents/km2/year\nkdeAssaults$zKM2Year <- kdeAssaults$z * nrow(dataAssaults) * 10^6 / nYears\nhist(kdeAssaults$zKM2Year,\n     xlab = \"Crime rate\",\n     ylab = \"Number of points\",\n     main = \"\")\n```\n\n::: {.cell-output-display}\n![Histogram of estimated crime intensity (incidents per km² per year)](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-26-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nThe histogram shows that a large number of areas have very low crime rates, but this histogram has a long right tail, meaning some parts of Chicago have high crime rates. Let's determine some nice breakpoints for our map.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbreaks <- kdeAssaults$zKM2Year |>\n   range() |>\n   pretty(n=10)\nbreaks\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]   0  20  40  60  80 100 120 140 160 180\n```\n\n\n:::\n:::\n\n\n\nNow we ask `isobands()` to find level sets for each value of `breaks`. That is, `isobands()` will find curves through x and y where the value of `zKM2Year` is close to each value of `breaks`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontourAssaults <- \n   isobands(kdeAssaults$x, \n            kdeAssaults$y, \n            kdeAssaults$zKM2Year,\n            levels_low  = breaks[-length(breaks)],\n            levels_high = breaks[-1]) |>\n   iso_to_sfg() |>        # convert to sf geometry object\n   st_sfc(crs = 26916) |> # convert to sf data column\n   st_sf(levels_low  = breaks[-length(breaks)], # convert to sf object\n         levels_high = breaks[-1],\n         geometry = _) |> \n   st_transform(4326)\n\n# show the result so far\ncontourAssaults |>\n   slice(-1) |> # drop the outer edge\n   st_geometry() |>\n   plot()\n```\n\n::: {.cell-output-display}\n![Isobands for Chicago assault incident rate](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-28-1.png){fig-pos='H'}\n:::\n:::\n\n\n\n\nLet's create a viridis color palette to shade the areas between these contour lines.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npal <- colorBin(\"viridis\",\n                domain = (contourAssaults$levels_low + \n                             contourAssaults$levels_high)/2,\n                bins = breaks,\n                pretty = FALSE)\n```\n:::\n\n\n\nNow we are ready to overlay a colored contour map on top of our Chicago leaflet map. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleaflet() |>\n  addTiles() |>\n  setView(lng = -87.8, lat = 41.85, zoom = 10) |>\n  addPolygons(\n    data = contourAssaults,\n    fillColor = ~pal((levels_low + levels_high)/2),\n    fillOpacity = 0.4,\n    color = \"#808080\",  # contour band edges\n    weight = 0.5,\n    opacity = 0.5,\n    label = ~paste0(\"Density: \",\n                    format(levels_low,  scientific=FALSE), \"-\",\n                    format(levels_high, scientific=FALSE)) |>\n       lapply(htmltools::HTML),\n    highlightOptions = \n       highlightOptions(weight = 2, bringToFront = TRUE)) |>\n  addLegend(\n    position = \"bottomright\",\n    pal = pal,\n    values = (contourAssaults$levels_low + \n                 contourAssaults$levels_high)/2,\n    title = \"Incidents / km² / year\",\n    opacity = 0.7)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nfile:///C:/Users/greg_/AppData/Local/Temp/Rtmp4k8bfh/file7abc3cc32584/widget7abc55369f1.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Chicago assault hot spots](07_Crime_Hotspot_Map_files/figure-pdf/leafletChicagoHotSpots-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nThe contours have been created over the Chicago bounding box, which ends up looking a little strange, with areas highlighted in Lake Michigan. We are going to intersect the contours with the data points' concave hull, a tight-fitting polygon that wraps a point set while allowing some inward dents. The `ratio` parameter sets how tight the fit is, roughly the fraction of the convex hull's area to keep. This leaflet map shows the convex hull (the smallest polygon that contains all the data points with no inward dents) in gray and the concave hull with ratio set to 0.2 in purple. We could use either one to trim the KDE contour map to places where assaults actually occur.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleaflet() |>\n   addTiles() |>\n   setView(lng = -87.8, lat = 41.85, zoom = 10) |>\n   addPolygons(\n      data = dataAssaults |> \n         st_geometry() |>\n         st_combine() |>\n         st_convex_hull() |>\n         st_buffer(200) |> # add a little extra at the edges\n         st_transform(crs = 4326),\n      color = \"#808080\",\n      weight = 0.5,\n      opacity = 0.9)  |>\n   addPolygons(\n      data = dataAssaults |> \n         st_geometry() |>\n         st_combine() |>\n         st_concave_hull(ratio=0.2) |>\n         st_buffer(200) |> # add a little extra at the edges\n         st_transform(crs = 4326),\n      color = \"purple\",\n      weight = 0.5,\n      opacity = 0.9,\n    highlightOptions = \n       highlightOptions(weight = 2, bringToFront = TRUE))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nfile:///C:/Users/greg_/AppData/Local/Temp/Rtmp4k8bfh/file7abc3c7d5980/widget7abc10e57c9d.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/leafletChicagoConcaveHull-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nAlso, let's make the hotspot map only highlight the hottest spots, those with a crime rate exceeding 80 incidents per km^2^ per year. Zoom in on some of the brightest areas to see what is in these high car theft areas.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlegendMid <- (contourAssaults$levels_low + \n                 contourAssaults$levels_high)/2\nlegendMid <- legendMid[legendMid > 80]\nlegendCol <- pal(legendMid)\nlegendLabs <- paste0(contourAssaults$levels_low, \"-\",\n                     contourAssaults$levels_high)\nlegendLabs <- legendLabs[5:9]\n\nleaflet() |>\n   addTiles() |>\n   setView(lng = -87.8, lat = 41.85, zoom = 10) |>\n   addPolygons(\n      data = contourAssaults |>\n         st_intersection(dataAssaults |> \n                            st_geometry() |>\n                            st_combine() |>\n                            st_concave_hull(ratio=0.2) |>\n                            st_buffer(200) |> # add a little extra at the edges\n                            st_transform(crs = 4326)) |>\n         filter(levels_low >= 80), # highlight just the hottest spots\n      fillColor = ~pal((levels_low + levels_high)/2),\n      fillOpacity = 0.4,\n      color = \"#808080\",\n      weight = 0.5,\n      opacity = 0.5,\n      label = ~paste0(\"Density: \",\n                      format(levels_low,  scientific=FALSE), \"-\",\n                      format(levels_high, scientific=FALSE)) |>\n         lapply(htmltools::HTML),\n      highlightOptions = \n         highlightOptions(weight = 2, bringToFront = TRUE)) |>\n   addLegend(\n      position = \"bottomright\",\n      colors = legendCol,\n      labels = legendLabs,\n      title = \"Incidents / km² / year\",\n      opacity = 0.7)\n```\n\n::: {.cell-output-display}\n![Chicago assault hot spots](07_Crime_Hotspot_Map_files/figure-pdf/leafletChicagoHotSpotsConcaveHull-1.png){fig-pos='H'}\n:::\n:::\n\n\n\n\n# Visualizing changes in hotspots over time\nLet's create hotspot maps with just motor vehicle theft, and let's make a map for each year between 2018 and 2024. Take a look at the `Date` column in our initial dataset. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbGetQuery(con, \"SELECT Date \n                 FROM crime\n                 LIMIT 5\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    Date\n1 07/29/2022 03:39:00 AM\n2 01/03/2023 04:44:00 PM\n3 08/10/2020 09:45:00 AM\n4 08/26/2017 10:00:00 AM\n5 09/06/2023 05:00:00 PM\n```\n\n\n:::\n:::\n\n\n\nWe want only the year part of these dates. The year starts with the 7^th^ character and has a length of 4 characters. So to extract just the year we can use the built-in SQL function `SUBSTR()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataCarTheft <- dbGetQuery(con, \"\n   SELECT Latitude, \n          Longitude,\n          SUBSTR(Date,7,4) AS year\n   FROM crime\n   WHERE PrimaryType='MOTOR VEHICLE THEFT' AND\n         Latitude  IS NOT NULL AND\n         Longitude IS NOT NULL AND\n         year >= 2018 AND year <= 2024\")\ndataCarTheft |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Latitude Longitude year\n1 41.76935 -87.61501 2022\n2 41.80890 -87.61814 2023\n3 41.76170 -87.71864 2023\n4 41.90969 -87.79601 2023\n5 41.76342 -87.70524 2023\n6 41.98896 -87.78277 2023\n```\n\n\n:::\n:::\n\n\n\n\nNow that we have the car theft data pulled out of our SQL database, convert to a spatial data frame with `st_as_sf()` and make a color palette for the range of car theft rates.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataCarTheft <- dataCarTheft |>\n   st_as_sf(coords = c(\"Longitude\", \"Latitude\"), \n            crs = 4326, # lat/long coordinate system\n            remove = FALSE) |>\n   st_transform(crs = 26916) \n\nbreaks <- seq(0, 140, by=20)\n\npal <- colorBin(\"viridis\",\n                domain = (breaks[-length(breaks)]+breaks[-1])/2,\n                bins = breaks,\n                pretty = FALSE)\n```\n:::\n\n\n\n\nWe then loop through the years 2018 to 2024, subsetting the data to one of those years at a time, and generate a hotspot map.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmaps <- lapply(2018:2024, \nfunction(year0) \n{\n   message(paste(\"Mapping\", year0))\n\n   xy <- dataCarTheft |>\n      filter(year == year0) |> # for just one year\n      st_coordinates() |>\n      data.frame()\n   h <- c(bandwidth.nrd(xy$X), bandwidth.nrd(xy$Y))  \n   kdeCarTheft <- kde2d(xy$X, xy$Y, n = 200, h = h)\n   \n   # no need to divide by year here... only one year of data\n   kdeCarTheft$zKM2Year <- kdeCarTheft$z * nrow(xy) * 10^6 \n   \n   contourCarTheft <- \n      isobands(kdeCarTheft$x, \n               kdeCarTheft$y, \n               kdeCarTheft$zKM2Year,\n               levels_low  = breaks[-length(breaks)],\n               levels_high = breaks[-1]) |>\n      iso_to_sfg() |>\n      st_sfc(crs = 26916) |>\n      st_sf(levels_low  = breaks[-length(breaks)],\n            levels_high = breaks[-1],\n            geometry = _) |>\n      st_transform(4326) # back to lat/long\n   \n   leaflet() |>\n      addTiles() |>\n      setView(lng = -87.8, lat = 41.85, zoom = 10) |>\n      addPolygons(\n         data = contourCarTheft |>\n            st_intersection(dataCarTheft |> \n                               st_geometry() |>\n                               st_combine() |>\n                               st_concave_hull(ratio=0.7) |>\n                               st_buffer(200) |>\n                               st_transform(crs = 4326)) |>\n            filter(levels_low >= 25), # highlight just the hottest spots\n         fillColor = ~pal((levels_low + levels_high)/2),\n         fillOpacity = 0.4,\n         color = \"#808080\",\n         weight = 0.5,\n         opacity = 0.5,\n         label = ~paste0(\"Density: \",\n                         format(levels_low,  scientific=FALSE), \"-\",\n                         format(levels_high, scientific=FALSE)) |>\n            lapply(htmltools::HTML),\n         highlightOptions = \n            highlightOptions(weight = 2, bringToFront = TRUE)) |>\n      addLegend(\n         position = \"bottomright\",\n         pal = pal,\n         values = (contourCarTheft$levels_low + \n                      contourCarTheft$levels_high)/2,\n         title = paste(year0, \"Incidents / km² / year\"),\n         opacity = 0.7)\n})\n\ndir.create(\"figs\", showWarnings = FALSE)\n\nyrs <- 2018:2024\npngs <- character(length(maps))\n\nfor (i in seq_along(maps)) {\n  htmlf <- tempfile(fileext = \".html\")\n  pngf  <- file.path(\"figs\", paste0(\"car-theft-\",yrs[i],\".png\"))\n  htmlwidgets::saveWidget(maps[[i]], htmlf, selfcontained = TRUE)\n  webshot2::webshot(htmlf, pngf, vwidth = 1200, vheight = 900, zoom = 2)\n  pngs[i] <- pngf\n}\n\nknitr::include_graphics(pngs)\n```\n\n::: {.cell-output-display}\n![](figs/car-theft-2018.png){fig-pos='H' width=6in}\n:::\n\n::: {.cell-output-display}\n![](figs/car-theft-2019.png){fig-pos='H' width=6in}\n:::\n\n::: {.cell-output-display}\n![](figs/car-theft-2020.png){fig-pos='H' width=6in}\n:::\n\n::: {.cell-output-display}\n![](figs/car-theft-2021.png){fig-pos='H' width=6in}\n:::\n\n::: {.cell-output-display}\n![](figs/car-theft-2022.png){fig-pos='H' width=6in}\n:::\n\n::: {.cell-output-display}\n![](figs/car-theft-2023.png){fig-pos='H' width=6in}\n:::\n\n::: {.cell-output-display}\n![](figs/car-theft-2024.png){fig-pos='H' width=6in}\n:::\n:::\n\n\n\n\nCar thefts spiked in between 2022 and 2024.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataCarTheft |> \n   count(year) |> \n   plot(n~year, data=_,\n        xlab = \"Year\",\n        ylab = \"Number of car thefts\")\n```\n\n::: {.cell-output-display}\n![](07_Crime_Hotspot_Map_files/figure-pdf/unnamed-chunk-33-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nThat spiked was fueled by thefts of Kia and Hyundai vehicles, which lacked passive immobilizer antitheft devices as standard equipment and lead to a social media trend describing how to steal Kia and Hyundai vehicles. \n\nMany cities other than Chicago have accessible incident-level data. You can easily modify the code here to make a hotspot map for Philadelphia, Los Angeles, Seattle, San Francisco, Baltimore, or Washington, DC.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}