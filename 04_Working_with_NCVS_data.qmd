---
title: "Working with National Crime Victimization Survey Data"
author:
- affiliation: University of Pennsylvania
  email: gridge@upenn.edu
  name: Greg Ridgeway
- affiliation: University of Pennsylvania
  email: moyruth@upenn.edu
  name: Ruth Moyer
- affiliation: University of Pennsylvania
  email: gohl@upenn.edu
  name: Li Sian Goh
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    theme: 
      dark: darkly
      light: default
    toc: true
    html-math-method: mathjax
  pdf:
    toc: true
    prefer-html: true
number-sections: true
editor_options: 
  chunk_output_type: console
bibliography: G:/My Drive/docs/Greg/articles/mybib.bib
---


<!-- In terminal -->
<!-- quarto render 04_Working_with_NCVS_data.qmd -->

<!-- git commit 04-* -m "commit message" -->
<!-- git status -->
<!-- git push -->



<!-- A function for automating the numbering and wording of the exercise questions -->
```{r echo=FALSE}
.counterExercise <- 0
.exerciseQuestions <- NULL
.exNum <- function(.questionText="") 
{
   .counterExercise <<- .counterExercise+1
   .exerciseQuestions <<- c(.exerciseQuestions, .questionText)
   .questionText <- gsub("@@", "`", .questionText)
   return(paste0(.counterExercise,". ",.questionText))
}
```




# Introduction

Through our work with NIBRS, we have already discussed reported crime. Nonetheless, not all crimes are reported to the police. Each year, under the guidance of the Bureau of Justice Statistics, the U.S. Census Bureau conducts the National Crime Victimization Survey (NCVS), a source of self-reported victimization data. The Census Bureau interviews a sample of people 12 years old or older about the number and characteristics of crime victimizations they experienced during the prior 6 months. 

In 2023 226,480 people in 142,028 households participated. The survey had a 63\% response rate for households and 82\% response rate for individuals. Households remain in the sample for $3\frac{1}{2}$ years completing interviews every 6 months, in person or by phone, for a total of seven interviews. The survey cost \$62M annually and required roughly 125,000 hours of uncompensated respondent time.

The NCVS contains information about nonfatal personal crimes, such as rape and robbery, as well as property crimes, such as burglary. Additional information about the NCVS can be found at the [BJS website](https://bjs.ojp.gov/data-collection/ncvs). To give a sense of the type of data that the NCVS contains, refer to the [Official 2023 BJS Crime Victimization report](https://bjs.ojp.gov/library/publications/criminal-victimization-2023).

# Acquiring the NCVS data

The University of Michigan consolidates the NCVS data into a format that is easily accessible in R. We will be using data collected in 2022 and 2023 to assemble a dataset that covers victimizations occurring in 2022. Since respondents are asked about crime in the previous six months, respondents completing surveys in May 2023 will still be reporting about crimes in December 2022.

First, we will download the NCVS 2022 data, [ICPSR 38603](https://www.icpsr.umich.edu/web/NACJD/studies/38603). Click on Download, select R, save the resulting file (called something like ICPSR_38603-V1.zip), extract the contents of the zipped file to a convenient folder, and give it a more understandable folder name, like NCVS2022. Repeat the process for downloading the NCVS 2023 data, [ICPSR 38962](https://www.icpsr.umich.edu/web/NACJD/studies/38962). New NCVS data tends to appear in mid-September. Typically we need to wait about nine months to get the results from the previous year.

After unzipping the NCVS files, you will find subfolders called DS0001, DS0002, DS0003, DS0004, and DS0005. 
```{r}
list.files("NCVS2022/",recursive = TRUE)
list.files("NCVS2023/",recursive = TRUE)
```
Inside each of these subfolders you see an R data file with the extension .rda. We will spend most of our attention on the contents of the DS0005 folder, which contains the "incident-level extract file." In each folder you will also find codebooks in pdf (and epub) format. The codebook is as important as it is tedious for understanding what is stored in the NCVS data. You should become familiar with the codebooks as soon as you can.

Let's start loading these datasets. We will skip the DS0001 subfolder, which contains basic survey information on the targeted addresses. The DS0002 folder contains data on the households included in the survey.
```{r}
load("NCVS2022/DS0002/38603-0002-Data.rda")
load("NCVS2023/DS0002/38962-0002-Data.rda")

# and let's give them nicer names
dataHH22 <- da38603.0002
dataHH23 <- da38962.0002
```
Take a peek at the first couple of rows
```{r}
#| message: false
library(dplyr)
library(tidyr)

dataHH22 |>
  head() |>
  select(V2001, YEARQ, IDHH, V2003, V2014, V2016, V2018, V2020,
         V2030, V2031, V2032, V2034, V2036, V2038, V2040A, V2127B, V2129) 
```
The 2022 household dataset has `r ncol(dataHH22)` columns. Instead of printing out all of them, here I just picked out 17 columns here. First off you can see that the column names are generally not helpful. That is where the codebook comes in handy. The codebook tells you what each variable means.

Somewhat hidden is a table linking column names to English explanations of what is in those columns. You can get to it by extracting the data frame's "attributes" with `attr()`.
```{r}
varsHH <- dataHH22 |>
   attr("variable.labels") |>
   data.frame() |>
   tibble::rownames_to_column() |>
   setNames(c("varname","details")) |>
   filter(!grepl("^HHREP", varname)) # exclude rows that start with HHREP
```
Now `varsHH` has two columns, the first with the column names and the second with the details. Let's pull up the 17 columns listed before.
```{r}
varsHH |>
   filter(varname %in% c("V2001","YEARQ","IDHH","V2003","V2014","V2016",
                         "V2018","V2020","V2030","V2031","V2032","V2034","V2036",
                         "V2038","V2040A","V2127B","V2129")) 
```
These are much more intelligible descriptions. "(S)MSA" stands for the Standard Metropolitan Statistical Areas, an outdated term. Today we call them simply MSAs. Minimum population has to be 50,000, but there is movement toward redefining as 100,000.

Note the first household record has `IDHH` equal to 1809000258543680568236125. We can load the respondent "person file" to see who in this household responded.

```{r}
# loading person-level data
load("NCVS2022/DS0003/38603-0003-Data.rda")
load("NCVS2023/DS0003/38962-0003-Data.rda")
dataPers22 <- da38603.0003
dataPers23 <- da38962.0003

# lookup respondents from this household
dataPers22 |>
  filter(IDHH=="1809000258543680568236125") |>
  select(!starts_with("PERREP"))  # drop all the PERREP weight columns
```
These two rows represent two surveys, six months apart of the same divorced, 62-63 year-old, white male. Let's look up another household.
```{r}
dataPers22 |>
  filter(IDHH=="1809000284384631568236124") |>
  select(!starts_with("PERREP"))
```
These rows represent two surveys occurring at the same time, one of the reference person, a married white female, and a second survey of her husband.

Let's grab the variable details as we did with the household data.
```{r}
#| results: "hold"
# Person file also has list of variable details
varsPers <- dataPers22 |>
   attr("variable.labels") |>
   data.frame() |>
   tibble::rownames_to_column() |>
   setNames(c("varname","details")) |>
   filter(!grepl("^(PERREP|PINTTYPE)", varname))
varsPers
```

There is an incident-level file that we will read in here. We are not going to look at it further, since much of the information in this file is also in the incident extract file.
```{r}
#| results: "hold"
load("NCVS2022/DS0004/38603-0004-Data.rda")
load("NCVS2023/DS0004/38962-0004-Data.rda")
dataInc22 <- da38603.0004
dataInc23 <- da38962.0004

dataInc22 |>
   select(IDHH, IDPER, V4014, V4529) |> 
   head()
```

Finally, we will load in the incident extract file and its associated variable details. This extract file merges in household-level and person-level information to the incident-level file, allowing you to connect person-level features with features of the victimizations they report.
```{r}
#| results: "hold"
# incident-level extract file
load("NCVS2022/DS0005/38603-0005-Data.rda")
load("NCVS2023/DS0005/38962-0005-Data.rda")
dataExt22 <- da38603.0005
dataExt23 <- da38962.0005

varsExt <- dataExt22 |>
   attr("variable.labels") |>
   data.frame() |>
   tibble::rownames_to_column() |>
   setNames(c("varname","details")) |>
   filter(!grepl("INCREPWGT|VICREPWGT", varname))
```

Let's take a look at a few of the reported crime victimizations. Here I will just pull the respondent's age, marital status, sex, general location, and crime type.
```{r}
dataExt22 |>
  select(V3014, V3015, V3018, V4022, V4529) |>
  slice(1:3)
```
Not all information from the household and person files are in the extract file, but many of the features that are likely to be of interest are there.

Now that the datasets are loaded and renamed, we can remove objects from our working environment that we no longer need. We can use `rm()` to accomplish this.
```{r}
#| results: 'hold'
rm(da38603.0002,da38603.0003,da38603.0004,da38603.0005,
   da38962.0002,da38962.0003,da38962.0004,da38962.0005)
```

# Combining 2022 and 2023 data

Here we are going to create a data frame containing all the reported incidents that *occurred* in 2022.
Take a look at the month and year of the reported crime incidents.
```{r}
dataExt22 |> count(V4015, V4014)
dataExt23 |> count(V4015, V4014)
```
Note that the 2022 NCVS reports on crimes that occurred in 2022 and 2021. Similarly, the NCVS 2023 reports on crimes that occurred in 2023 and 2022. Remember that the NCVS surveys as respondents about any victimizations from the prior 12 months. We will going to stack the 2022 and 2023 incident extract data frames and then filter it to exclude 2021 and 2023.

`bind_rows()` stacks data frames on top of each other, useful when combining two datasets that have the same structure. First we will check that they have the same columns in them.
```{r}
identical(names(dataExt22), names(dataExt23))
```
Good so far! Now let's try to stack them.
```{r}
#| error: true
dataExt <- dataExt22 |>
  bind_rows(dataExt23)
```
Hmmm... R is complaining about `V2061`. Note that it specifically complains that one data frame has `V2061` stored as a factor (a categorical variable) and the other one has it stored as a double, a decimal number.
```{r}
dataExt22 |> count(V2061)
dataExt23 |> count(V2061)
```
What is `V2061` anyway?
```{r}
varsExt |> filter(varname=="V2061")
```
This reports on who reported on behalf of an unavailable respondent. Not really important for us so let's drop this one by using `select(-V2061)` on both data frames.

```{r}
#| error: true
dataExt <- dataExt22 |>
  select(-V2061) |>
  bind_rows(dataExt23 |>
              select(-V2061))
```
Ughh. Now it is complaining about `V4126`.
```{r}
varsExt |> filter(varname=="V4126")
dataExt22 |> count(V4126)
dataExt23 |> count(V4126)
```
In the codebook we can find the full question: "Q.33.3 Which injuries were caused by a weapon OTHER than a gun or knife?". This seems like a potentially interesting question that I probably do not want to discard. The issue is that no 2023 respondent said there was a third weapon that injured them. In 2022 `V4126` was stored as a factor, but in 2023, since they are all missing, R defaulted to numeric (double). We can fix this by just telling R to convert the 2023 data into a factor.
```{r}
#| error: true
dataExt <- dataExt22 |>
  select(-V2061) |>
  bind_rows(dataExt23 |>
              select(-V2061) |>
              mutate(V4126=as.factor(V4126)))
```
Dammit! Now it is complaining about `V4313`. What is the problem with this one? Again we have a problem with 2022 storing as a factor and 2023 storing as a double.
```{r}
varsExt |> filter(varname=="V4313")
dataExt22 |> count(V4313)
dataExt23 |> count(V4313)
```
This column answers "Besides the respondent, which household member(s) owned the (property/money) the offender tried to take?" In 2022, the only responses were missing or #6. In 2023, the responses were missing, 4, or 5. These should be numbers since they are supposed to link respondents in the same household affected by the theft. The approach I'll take is to use `case_match()` telling R to change the 2022 "(006) 6" response to a regular 6.
```{r}
#| error: true
dataExt <- dataExt22 |>
  select(-V2061) |>
  mutate(V4313 = case_match(V4313,
                            "(06) 6" ~ 6,
                            .default = NA)) |>

  bind_rows(dataExt23 |>
              select(-V2061) |>
              mutate(V4126=as.factor(V4126)))
```
Will this ever stop?!?!? Another double in one year and factor in another year, this time affecting `V4357A` asking about handguns.
```{r}
varsExt |> filter(varname=="V4357A")

dataExt22 |> count(V4357A)
dataExt23 |> count(V4357A)
```
You might have seen this word "Residue" show up before. For the NCVS, BJS records "Residue" when there is a data entry error resulting in an out-of-range code, an incorrect or unusable answer by the respondent, or the absence of an entry for a question that should have been asked. Sometimes you might also see "Out of universe/blank." This happens when a value is outside the range of questions to be answered. For example, "Received Medical Care for Injury," only victims who report being injured are asked whether they received medical care. All other victims skip this question.

I will solve this issue by recoding the 2023 values to numeric values.
```{r}
dataExt <- dataExt22 |>
  select(-V2061) |>
  mutate(V4313 = case_match(V4313,
                            "(06) 6" ~ 6,
                            .default = NA)) |>
  bind_rows(dataExt23 |>
              select(-V2061) |>
              mutate(V4126 = as.factor(V4126),
                     V4357A = case_match(V4357A,
                                         "(001) 1" ~ 1,
                                         "(998) Residue" ~ 998,
                                         .default=NA)))
```
Success! Now let's check that all is okay now.
```{r}
dataExt |> count(V4126)
dataExt |> count(V4313)
dataExt |> count(V4357A)
```

Remember that we still have data in here from 2021 and 2023.
```{r}
table(dataExt$V4015)
```
We are just going to focus on 2022.
```{r}
dataExt <- dataExt |> filter(V4015==2022)
```
Note that BJS official reports generally classify by the year of the survey and not by the year of the crime.

# BJS modifications and survey weights

Some respondents report crime victimizations that occurred outside of the United States.
```{r}
# V4022 - IN WHAT CITY, TOWN, VILLAGE.
dataExt |> count(V4022)
```
The BJS convention is to exclude these crimes in official reports (see 2023 User Guide, page 21).

```{r}
dataExt <- dataExt |>
  filter(is.na(V4022) | V4022!="(1) Outside U.S.")
```

Some crimes happen in a series. For example, a respondent may report on regular domestic abuse that happened numerous times over the last six months. Each incident of domestic abuse is a victimization, but the BJS convention is to include up to 10 occurrences for crimes reported as a series (2023 User Guide, pages 18-19).

Variable `V4016` records the answer to "Altogether, how many times did this type of incident happen during the last 6 months?" and variable `V4019` documents "Can you (respondent) recall enough details of each incident to distinguish them from each other?"

Note that the coding of `V4016` has 997 representing "Don't know" and 998 representing "Residue". These are not counts of victimizations. The logic in the `case_when()` statement below checks for counts between 11 and 996 and sets the value of `V4016` to 10 in that case.
```{r}
dataExt <- dataExt |>
  mutate(V4016 = case_when(
    V4019=="(2) No (is series)" & V4016>=11 & V4016<=996 ~ 10,
    V4016 >= 997 ~ NA,
    .default=V4016))
```

The NCVS sampling design oversamples respondents in places more likely to have crime victimization. This makes the sampling effort more efficient. Otherwise, a purely random sample would contact a lot of people who had no victimization to report. As a result, the raw data from the NCVS do not reflect crime victimization in the United States. We must use the NCVS sampling weights to undo the oversampling of crime victims. 

Constructing the sampling weights is a complex process (see the User Guide "Weights Details" section starting on Page 22). The NCVS sampling weights adjusts for six factors. From the User Guide:

1.  *Base weight*: The inverse of the national sampling rate for the stratum of that unit (person or household).

2.  *Weighting control*: Adjusts for any sub-sampling due to unexpected events in the field, such as new construction, area segments larger than anticipated, and other deviations from the overall stratum sampling rate.

3.  *Household non-interview adjustment*: Adjusts for nonresponse at the household-level by increasing the weights of interviewed households most similar to households not interviewed in terms of race, MSA status of residence, and urban/suburban/rural status of residence. This inflates the weight value assigned to interviewed households so that they represent themselves and non-interviewed households. The non-interviewed cases are assigned a weight of zero, thereby excluding them from population estimates.

4.  *Within-household non-interveiew adjustment*: Adjusts for non-response at the person-level by increasing the weight of interviewed persons most similar to persons not interviewed in terms of region, age, race, sex, and household composition. The adjustment inflates the weight value assigned to completed interviews, so that they represent themselves and sampled individuals who were not interviewed. The non-interviewed cases are assigned a weight of zero. 

5.  *First stage ratio estimates factor*: Adjusts for differences between characteristics of the sample non-self-representing (NSR) primary sampling units (PSUs) and independent measures of the population NSR PSUs. (For self-representing PSUs this factor is set to 1). This factor adjusts for PSU differences on region, MSA status, urban/suburban/rural status, and racial composition.

6.  *Second stage ratio estimate factor*: A post-stratification factor defined for each person to adjust for the difference between weighted counts of persons (using the above five weight components) and independent estimates of the number of persons, within certain age by race by sex categories. These independent estimates are based on the Census population controls adjusted for the undercount.

Fortunately for us, the variable `SERIES_WEIGHT` captures all these adjustments and contains the weight that BJS uses for its official reports. It includes the adjustment for capping series crimes at 10.

::: callout-tip
## Always use the sampling weights

Importantly, every calculation you do with the NCVS must involve the weights. This includes weighted means, weighted percentages, and weighted counts. Even plots and figures should use the weights.
:::

Where you would normally compute a sample mean as $\frac{\sum x_i}{n}$, the weighted mean is
$$
\frac{\sum w_ix_i}{\sum w_i}
$$
For a weighted percentage, total all the weights for respondents with the particular feature divided by the total weight. For some plots there is not an obvious way to accommodate the sampling weights. In those cases we can sample with replacement with probabilities proportional to the sampling weights and plot the sampled points.

# Tabulating victimizations

First, we need to be clear about what we are counting. BJS will report on *victimizations* and *incidents*. Victimizations count the number of times a US person was victimized. Incidents count the number of times a crime incident occurred and those incidents could involve multiple victims. BJS reports largely focus on criminal victimizations.

We can start but just asking the NCVS data how many criminal victimizations there were in 2022. We compute that as the sum of all the weights.
```{r}
sum(dataExt$SERIES_WEIGHT)
```
This means that the NCVS estimates that there were `r prettyNum(sum(dataExt$SERIES_WEIGHT), big.mark = ",")` criminal victimizations in the United States in 2022.

Let's take a closer look at what kinds of victimization occurred. Note that this code breaks the dataset into groups based on the reported crime type, `V4529`, and computes the total weight associated with each of those crime categories.
```{r}
dataExt |>
  group_by(V4529) |> # crime type
  summarize(total = sum(SERIES_WEIGHT)) |>
  print(n=Inf)
```
The first 20 crime types listed are the violent crimes and the remainder are property crimes. Let's extract the two-digit code between the parentheses so that we can classify crime types as violent or property. First, I will run a little test code to make sure my regular expression and the crime type classification works correctly.
```{r}
dataExt |>
   mutate(crimeCode = gsub(".([0-9][0-9]).*", "\\1", V4529),
          crimeType = ifelse(crimeCode <= 20, "violent", "property")) |>
   select(V4529, crimeCode, crimeType) |>
   head()
```
That all looks correct, so now I can move on to the tabulation.
```{r}
a <- dataExt |>
   mutate(crimeCode = gsub(".([0-9][0-9]).*", "\\1", V4529),
          crimeType = ifelse(crimeCode <= 20, "violent", "property")) |>
   group_by(crimeType) |>
   summarize(estTotal = sum(SERIES_WEIGHT))
a
```
In 2022, there was an estimated `r prettyNum(a$estTotal[2], big.mark=",")` violent crimes and `r prettyNum(a$estTotal[1], big.mark=",")` property crimes.

We can summarize other categories, like car thefts, attempted and completed.
```{r}
dataExt |>
  filter(V4529 %in% c("(40) Motor veh theft",
                      "(41) At mtr veh theft")) |>
  summarize(sum(SERIES_WEIGHT))
```

Measuring sexual assault has been complicated by numerous, sometimes major, changes (improvements, more precisely) in the definitions and data collection methods (e.g. question wording). The Uniform Crime Report made a major change to the the [definition of rape changed](https://le.fbi.gov/cjis-division/cjis-link/ucr-program-changes-definition-of-rape) in 2013. The NCVS's most recent change was in 2024. See @FisherGross2025 for an extended discussion and timeline of the changes.

Here is the NCVS estimate of the number of sexual assaults (attempted and completed) for 2022.
```{r}
dataExt |>
  filter(V4529 %in% c("(01) Completed rape",
                      "(02) Attempted rape")) |>
  summarize(sum(SERIES_WEIGHT))
```

NCVS official reports combine all rape and sexual assaults. There are a lot of crime categories that describe sexual assaults.
```{r}
unique(dataExt$V4529)
```
Let's count any that have the word "rape" and any that have the word "sex" (sometimes capitalized). Here are the ones that BJS counts in this category.
```{r}
dataExt |>
  filter(grepl("rape|[Ss]ex", V4529)) |>
  distinct(V4529)
```
The estimated number of all sexual assaults in 2022 in the United States is
```{r}
dataExt |>
  filter(grepl("rape|[Ss]ex", V4529)) |>
  summarize(sum(SERIES_WEIGHT))
```
Since 2019 we have had no national crime estimates based on police reports.  

# Calculating victimization by demographics

In the remainder of these notes, we will examine relationships between victimization and the respondents' features, like age (`V3014`), marital status (`V3015`), and sex (`V3018`). To make the code more clear, let's give these variables more intelligible names.
```{r}
dataExt <- dataExt |>
  rename(age=V3014, marital=V3015, sex=V3018)
```
Perhaps we are interested in which crimes disproportionately affect men and which disproportionately affect women. Start by tabulating crime type by sex.
```{r}
dataExt |>
  group_by(V4529, sex) |>
  summarize(count=sum(SERIES_WEIGHT)) |>
   print(n=Inf)
```
R produces a long narrow table. This format is sometimes useful, particularly when merging data frames. However, in this case having a table with counts for men and women side-by-side would be easier to absorb. `pivot_wider()` will swing the sex column into two side-by-side columns. 
```{r}
dataExt |>
  group_by(V4529, sex) |>
  summarize(count=sum(SERIES_WEIGHT)) |>
  pivot_wider(names_from=sex,
              values_from=count,
              values_fill = 0) |> # fill in NAs with 0
  print(n=Inf)
```
Lastly, let's normalize the columns so that they add up to 100\%, giving us the distribution of crime types within each sex.
```{r}
dataExt |>
  group_by(V4529, sex) |>
  summarize(count=sum(SERIES_WEIGHT)) |>
  ungroup() |>  # without this, percentages are computed within crime type
  pivot_wider(names_from  = sex,
              values_from = count,
              values_fill = 0) |>
  rename(male=`(1) Male`, female=`(2) Female`) |>
  mutate(male  =100*male/sum(male),
         female=100*female/sum(female),
         ratio=female/male) |>
  arrange(desc(ratio)) |>
  print(n=Inf)
```
Sexual assaults disproportionately affect women, while pocket picking and attempted robbery involving assaults disproportionately affect men.

The sequence `group_by()`, `summarize()`, `ungroup()` is so common that there is an alternative way to do the same calculation more compactly with `.by` in `summarize()`. A frequent R error is to use `group_by()`, then forget that the data is still grouped, and continue to do calculations unaware that they are occurring only within groups. The `.by` argument also helps avoid this error.
```{r}
# can reduce the group_by, summarize, ungroup with .by
dataExt |>
  summarize(count=sum(SERIES_WEIGHT),
            .by = c(V4529, sex)) |> # eliminates group/ungroup
  pivot_wider(names_from  = sex,
              values_from = count,
              values_fill = 0) |>
  rename(male=`(1) Male`, female=`(2) Female`) |>
  mutate(male  =100*male/sum(male),
         female=100*female/sum(female),
         ratio=female/male) |>
  arrange(desc(ratio)) |>
  print(n=Inf)
```

We can do a similar calculation by age. First, let's discretize age into some fixed age bins. Then, we can repeat the same calculation to learn about victimization differences by age. I have sorted the results by the 18-24 age category, but you can change it to your age category if you wish.

```{r}
# can reduce the group_by, summarize, ungroup with .by
dataExt |>
   mutate(ageGroup = 
             cut(age, 
                 breaks = c(0, 17, 24, 34, 49, 64, Inf),
                 labels = c("12-17","18-24","25-34",
                            "35-49","50-64","65+"))) |>
   summarize(count=sum(SERIES_WEIGHT),
             .by = c(V4529, ageGroup)) |>
   pivot_wider(names_from  = ageGroup,
               values_from = count,
               values_fill = 0,
               names_sort = TRUE) |> # keep age groups ordered
   # apply the same function to every column, except V4529
   mutate(across(-V4529, function(x) 100*x/sum(x))) |>
   arrange(desc(`18-24`)) |> # you can change to your age group
   print(n=Inf)
```

Since we have made many changes to the dataset, I find it useful to save the final version. This way I can simply `load()` the data again later and know that it already has all the edits and changes that I have made.
```{r}
save(dataExt, file="NCVS2022.RData", compress=TRUE)
```
