---
title: "Introduction to SQL"
author:
- affiliation: University of Pennsylvania
  email: gridge@upenn.edu
  name: Greg Ridgeway
- affiliation: University of Pennsylvania
  email: moyruth@upenn.edu
  name: Ruth Moyer
- affiliation: University of Pennsylvania
  email: gohl@upenn.edu
  name: Li Sian Goh
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    css: htmlstyle.css
---

<!-- HTML YAML header Ctrl-Shift-C to comment/uncomment -->

<!-- --- -->
<!-- title: "Introduction to SQL" -->
<!-- author: -->
<!-- - Greg Ridgeway (gridge@upenn.edu) -->
<!-- - Ruth Moyer (moyruth@upenn.edu) -->
<!-- - Li Sian Goh (gohl@upenn.edu) -->
<!-- date: "`r format(Sys.time(), '%B %d, %Y')`" -->
<!-- output: -->
<!--   pdf_document: -->
<!--     latex_engine: pdflatex -->
<!--   html_document: default -->
<!-- fontsize: 11pt -->
<!-- fontfamily: mathpazo -->
<!-- --- -->
<!-- PDF YAML header Ctrl-Shift-C to comment/uncomment -->


<!-- Make RMarkdown cache the results -->
```{r echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=TRUE, cache.lazy=FALSE, out.width='100%')
```

<!-- A function for automating the numbering and wording of the exercise questions -->
```{r echo=FALSE}
.counterExercise <- 0
.exerciseQuestions <- NULL
.exNum <- function(.questionText="") 
{
   .counterExercise <<- .counterExercise+1
   .questionText <- gsub("@@", "`", .questionText)
   .exerciseQuestions <<- c(.exerciseQuestions, .questionText)
   return(paste0(.counterExercise,". ",.questionText))
}
.exQ <- function(i)
{
   return( paste0(i,". ",.exerciseQuestions[i]) )
}
```

<!-- To run this, first download the latest Chicago crime data from -->
<!-- https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2 -->
<!-- and put the file in the current folder -->

# Introduction

Some datasets are far too large for R to handle by itself. Structured Query Language ("SQL") is a widely used international standard language for managing data stored in a relational databases management system. A relational database management system itself is an approach to managing data using a structure that can be contrasted against the 'flat file' approach we have been using thus far with R. Why use SQL? R doesn't work very well with really huge datasets. A relational database management system offers a way of storing large amounts of information more efficiently and reducing the size of the dataset that we are working with. There are numerous relational database management systems such as Oracle, Microsoft Access, and MySQL. We are going to use [SQLite](https://www.sqlite.org/index.html), which is probably the most widely deployed database system. SQLite is in your phone, car, airplanes, thermostats, and numerous applicances. We are going to hook up SQLite to R so that R can handle large datasets.

These are some basic clauses in a SQL query that we will explore:

SELECT    	fields or functions of fields
INTO      	results table
FROM      	tables queried
WHERE     	conditions for selecting a record
GROUP BY  	list of fields to group
ORDER BY  	list of fields to sort by

However, before being able to use SQL as a tool in R, it will first be necessary to install the `sqldf` package.

```{r comment="", results='hold', cache=FALSE} 
library(sqldf)
```

# Getting the data into proper form

We will be working with Chicago crime data, which is accessible comma separated value (csv) format. Before we can even being learning SQL, we are going to have to do a fair bit of work to acquire the dataset, format it so that it's ready for SQLite, and then load it into the SQLite database.

Navigate to the Chicago open data website to get the [data](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2). Click the "Export" button and select the "CSV" option, or directly download from [here](https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD)

The Chicago crime data is huge, more than 1.5Gb. It contains about 7 million records on all crimes reported to the Chicago police department since 2001. R does not handle really large datasets well. By using SQL, you will learn how to more efficiently work with large datasets and learn a data language that is used absolutely everywhere.

Let's use `scan()` to just peek at the first three rows of the file.

```{r comment="", results='hold'} 
scan(what="",file="Crimes_-_2001_to_present.csv",nlines=5,sep="\n")
```
`scan()` is a very basic R function that reads in plain text files. We've told it to read in text (`what==""`), the name of the file, to only read in 5 lines (`nlines=5`), and to start a new row whenever it reaches a line feed character (`sep="\n"`). Using `scan()` without `nlines=5` would cause R to try to read in the whole dataset and that could take a lot of time and you might run out of memory.

You can see that the first row contains the column names. The second row contains the first reported crime in the file. You can see date and time, address, crime descriptions, longitude and latitude of the crime, and other information.

Importantly, SQLite is very particular about the formatting of a file. It can easily read in a csv file, but this dataset has some commas in places that confuse SQLite.

For example, there is a row in this file that looks like this:

```{r comment="", results='hold', echo=FALSE} 
print("10000153,HY189345,03/18/2015 12:20:00 PM,091XX S UNIVERSITY AVE,0483,BATTERY,AGG PRO.EMP: OTHER DANG WEAPON,\"SCHOOL, PUBLIC, BUILDING\",true,false,0413,004,8,47,04B,1185475,1844606,2015,02/10/2018 03:50:01 PM,41.728740563,-87.596150779,\"(41.728740563, -87.596150779)\"")
```

You see that the location description for this crime is `"SCHOOL, PUBLIC, BUILDING"`. Those commas inside the quotes are going to cause SQLite problems. SQLite is going to think that `SCHOOL`, `PUBLIC`, and `BUILDING` are all separate columns rather than one columns describing the location.

To resolve this, we're going to change all the commas that separate the columns into something else besides commas, leaving the commas in elements like `"SCHOOL, PUBLIC, BUILDING"` alone. It does not matter what we use to separate the fields, but it should be an unusual character that would not appear anywhere else in the dataset. Popular choices in the vertical bar (`|`) and the semi-colon (`;`). So let's take a slight detour to find out how to convert a comma-separated file into a semi-colon separated file. 

You'll know if you need to convert your file if, when you try to set up your SQL database, you receive an error message about an "extra column."

We're going to use a `while` loop to read in 1,000,000 rows of the our data file at a time. R can handle 1,000,000 rows. With the 1,000,000 rows read in, we'll use a regular expression to replace all the commas used for separating columns with semi-colons. Then we'll write out the resulting cleaned up rows into a new file. It is a big file so this code can take a few minutes to run to completion.

```{r comment="", results='hold', echo=FALSE, R.options=list(scipen=999)} 
infile  <- file("Crimes_-_2001_to_present.csv", 'r')       # 'r' for 'read'
outfile <- file("Crimes_-_2001_to_present-clean.csv", 'w') # 'w' for 'write'

cLines <- 0 # just a counter for the number of lines read

# read in 1000000 lines. keep going if more than 0 lines read
while ((length(a <- readLines(infile, n=1000000)) > 0))
{
   cLines <- cLines + length(a) # increase the line counter
   print(cLines)
   # remove any semi-colons if they are there
   a <- gsub(";", "", a)
   # use ?= to "lookahead" for paired quotes
   a <- gsub("(,)(?=(?:[^\"]|\"[^\"]*\")*$)", ";", a, perl=TRUE)
   # write the cleaned up data to storage
   writeLines(a, con=outfile)
}
close(infile)
close(outfile)
```

Now, let's take a look at the first five lines of the new file we just created.
```{r comment="", results='hold'} 
scan(what="",file="Crimes_-_2001_to_present-clean.csv",nlines=5,sep="\n")
```
You now see that semi-colons separate the columns rather than commas. That previous record that had the location description "SCHOOL, PUBLIC, BUILDING" now looks like this:
```{r comment="", results='hold', echo=FALSE} 
print("10000153;HY189345;03/18/2015 12:20:00 PM;091XX S UNIVERSITY AVE;0483;BATTERY;AGG PRO.EMP: OTHER DANG WEAPON;\"SCHOOL, PUBLIC, BUILDING\";true;false;0413;004;8;47;04B;1185475;1844606;2015;02/10/2018 03:50:01 PM;41.728740563;-87.596150779;\"(41.728740563, -87.596150779)\"")
```
Note that the commas are still there inside the quotes. Now we will be able to tell SQLite to look for semi-colons to separate the columns.

# Setting up the Database

Now that the file containing the data is ready, we can load it into SQLite. SQLite has its own way of storing and managing data. It can store multiple tables containing data in a single database. First, we'll tell SQLite to create a new database that we will call `chicagocrime`. Then we will tell SQLite to load our data file into a table called `crime`.

The next step is to set up the SQL database. The following lines of code will set up the database for you.  Make sure that your path is set correctly so that your database will be stored in the correct folder that you wish to work from. You will know if the database has been successfully set up if the database (stored as a .db file) is greater than 0 KB.  there is no reason to run these lines of code again. 

```{r comment="", results='hold'}
# create a connection to the database
con <- dbConnect(SQLite(), dbname="chicagocrime.db")

# peek at the first few rows of the dataset
a <- read.table("Crimes_-_2001_to_present-clean.csv",
                sep=";",nrows=5,header=TRUE)
# ask SQLite what data type it plans to use to store each column (eg number, text)
variabletypes <- dbDataType(con, a)
# make sure these features are stored as TEXT
variabletypes["IUCR"] <- "TEXT"
variabletypes["Ward"] <- "TEXT"
variabletypes["District"] <- "TEXT"
variabletypes["Community.Area"] <- "TEXT"

# just in case you've already created a "crime" table, delete it
if(dbExistsTable(con, "crime")) dbRemoveTable(con, "crime")
# import the data file into the database
dbWriteTable(con, "crime",                         # create crime table   
             "Crimes_-_2001_to_present-clean.csv", # from our cleaned up file
             row.names=FALSE,
             header=TRUE,                          # first row has column names
             field.types=variabletypes,            
             sep=";")                              # columns separated with ;
# does the table exist?
dbListTables(con)
# a quick check to see if all the columns are there
dbListFields(con,"crime")
# disconnect from the database to finalize
dbDisconnect(con)
```

Once you've successfully set up your database, there is no reason to run these lines of code again. You should never again need to turn commas into semi-colons or run `dbWriteTable()`. Instead, every time you want to work with your database, you can simply need to reconnect to the database with: 
```{r comment="", results='hold', cache=FALSE} 
con <- dbConnect(SQLite(), dbname="chicagocrime.db")
```
(Note that if you're keeping you are using a cloud-based backup service like iCloud, OneDrive, or Google Drive, you might need to wait until your "db" file has completely "synced" before you can access your database.)

# SQL queries
You've now created a database (called "chicagocrime.db") containing a table called "crime" that contains those 7 million crime records. 

Two important clauses with an SQL query are `SELECT` and `FROM`. Unlike R, SQL queries are not case-sensitive. Unlike in R, the column names in SQL aren't case-sensitive. So if we were to type "SELECT" as "select" or "Description" as "dEsCrIpTiOn", the SQL query would do the same thing. However, the tradition is to type SQL keywords in all uppercase to make it easier to distinguish them from table and column names.

The `SELECT` clause tells SQL which columns in particular you would like to see. The `FROM` clause simply tells SQL from which table it should pull the data. In this query, we are interested in only the `ID` and `Description` columns.  

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                   SELECT ID, Description
                   FROM crime")
fetch(res, n = 10) # just the first 10 rows
dbClearResult(res)
```

What we've done here is to create an object called `res`. `res` contains the results of our query. Then, we `fetch` those results. Within `fetch`, we set `n=10` so the first 10 rows are displayed. By convention, setting `n=-1`, will display all your rows. Really large SQL queries can be memory-intensive. So if your dataset is over 25 lines long (which it probably is.....that's why you're using SQL!), you have to make sure that you set the value in the fetch line to something reasonable to display.

However, suppose that your dataset is over 1 million rows, and you want to work with all of them.  You can set the `fetch` line to something like `mydata <- fetch(res, n=-1)`. 

`dbClearResult(res)` tells SQLite that we are all done with this query. We've retrieved the first 10 lines. SQLite is standing by with another 7 million rows to show us, but `dbClearResult(res)` tells SQLite that we are no longer interested in this query and it can clear out whatever it has stored for us.

In the previous SQL query we just asked for `ID` and `Description`. Typing out all of the column names would be tiresome, so SQL lets you use a `*` to select all the columns. If we want to look at the first 10 rows but all of the columns, we would use this query:
```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                   SELECT *
                   FROM crime")
fetch(res, n = 10) # just the first 10 rows
dbClearResult(res)
```

Just as `SELECT` filters the columns, the `WHERE` clause filters the rows. Note the use of `AND` and `OR` in the `WHERE` clause. As you might intuitively guess, `AND` and `OR` are logical operators that help us further filter our rows. Here we select three columns: `ID`, `Description`, and `Location.Description`. Also, we want only rows where 
* the value in the `Beat` column is "611"
* the value in the `Arrest` column is "true"
* the value in the `ICUR` column is either "0486" or "0498"

Importantly, note the use of single (not double) quotation marks in the `WHERE` line. The reason is that if we used double quotes, then R will think that the double quote signals the end of the query. Also note that `Location.Description` has a period in its name. The period has a special meaning in SQL that we will discuss later. We use the square brakets around the name to "protect" the column name, telling SQL to treat it as a column name.

Also, note how we set the `fetch()` line to the variable `a`. 

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                   SELECT ID, Description, [Location.Description]
                   FROM crime
                   WHERE ((Beat=611) AND 
                         (Arrest='true')) AND
                         ((IUCR='0486') OR (IUCR='0498'))")
a <- fetch(res, n = -1) # all the rows
dbClearResult(res)
# show the first few rows of the results
a[1:3,]
```

# Exercises

`r .exNum('Select records from Beat 234')`

`r .exNum('Select Beat, District, Ward, and Community Area for all "ASSAULT"s')`. Remember that, since `Primary.Type` has a period in its name, you need to use square brackets like `[Primary.Type]`

`r .exNum('Select records on assaults from Beat 234')`

`r .exNum('Make a table in R of the number of assaults (IUCR 0560) by Ward')`

# More SQL clauses

We've already covered SQL clauses `SELECT`, `WHERE`, and `FROM`. The SQL function `COUNT(*)` and `GROUP BY` are also very useful. For example, the following query counts how many assaults (IUCR 0560) occurred by ward. `COUNT()` is a SQL "aggregate" function, a function that performs a calculation on a group of values and returns a single number. Other SQL aggregate functions include `AVG()`, `MIN()`, `MAX()`, and `SUM()`. This query will group all the records by `Ward` and then apply the aggregate function `COUNT()` and report that value in a column called `crimecount`.

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
   SELECT COUNT(*) AS crimecount,
          Ward
   FROM crime
   WHERE IUCR='0560'
   GROUP BY Ward")
a <- fetch(res, n = -1)
dbClearResult(res)

print(a)
```

The `GROUP BY` clause is critical. If you forget it then the result is not well defined. That is, different implementations of SQL may produce different results. The rule you should remember is that "every non-aggregate column in the `SELECT` clause should appear in the `GROUP BY` clause." Here `Ward` is not part of the aggregate function `COUNT()` so it must appear in the `GROUP BY` clause.

# Practice exercises
`r .exNum('Count the number of crimes by @@Primary.Type@@')`

`r .exNum('Count the number of crimes resulting in arrest')`

`r .exNum('Count the number of crimes by @@Location.Description@@. @@LocationDescription@@ is the variable that tells us where (e.g., a parking lot, a barbershop, a fire station, a CTA train, or a motel) a crime occurred')`



# More SQL

`MAX`, `MIN`, `SUM`, `AVG` are common (and useful) aggregating functions. The `ORDER BY` clause sorts the results for us. It's the SQL version of the `sort()` command. Here is an illustration that gives the range of beat numbers in each policing district.

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
   SELECT MIN(Beat) AS min_beat,
          MAX(Beat) AS max_beat,
          District
   FROM crime
   GROUP BY District
   ORDER BY District")
fetch(res, n = -1)
dbClearResult(res)
```
Remember that the `GROUP BY` clause should include every element of the `SELECT` clause that is not involved with an aggregate function. We have `MIN()` and `MAX()` operating on `Beat`, but `District` is on its own and should be placed in the `GROUP BY` clause.

Let's look at our `Latitude` and `Longitude` columns (which, as we find in a subsequent section, will be extremely useful for mapping data points). The following query will give unexpected results.

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
   SELECT MIN(Latitude)  AS min_lat,
          MAX(Latitude)  AS max_lat,
          MIN(Longitude) AS min_lon,
          MAX(Longitude) AS max_lon,
          District
   FROM crime
   GROUP BY District
   ORDER BY District")
fetch(res, n = -1)
dbClearResult(res)
```
The first problem is that we have some rows with blank values in `Longitude` and `Latitude`. Here are some of them.
```{r comment="", results='hold'} 
fetch(dbSendQuery(con,"SELECT * FROM crime WHERE Longitude=''"), n=3)
dbClearResult(res)
```
Note the `X.Coordinate` and the `Y.Coordinate` columns. They should give the location per the State Plane Illinois East NAD 1983 projection, but in these rows they are empty.

The second problem is that there are minimum latitudes of 36.61945 and minimum longitudes of -91.68657. That's near the Missouri/Arkansas border 500 miles south of Chicago!
```{r comment="", results='hold'} 
fetch(dbSendQuery(con,"SELECT * FROM crime where Latitude<36.61946"), n=3)
dbClearResult(res)
```
Note that missing `X.Coordinate` and `Y.Coordinate` are getting mapped to some place far from Chicago.

We can tell SQLite to make the empty or missing values `NULL`, a more proper way to encode that these rows have missing coordinates. The `UPDATE` clause edits our table. R will read in `NULL` values as `NA`. After we do the update, we can rerun the `MIN()`, `MAX()` query. The dataset also has some latitudes and longitudes that are very close to 0 (and Chicago is quite far from the equator), but not exactly 0. We can make those `NULL` as well. We're also going to fix the `X.Coordinate` and `Y.Coordinate` columns.

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
   UPDATE crime SET Latitude=NULL
   WHERE (Latitude='') OR (ABS(Latitude-0.0) < 0.01) OR (Latitude < 36.7)")
dbClearResult(res)
res <- dbSendQuery(con, "
   UPDATE crime SET Longitude=NULL
   WHERE (Longitude='') OR (ABS(Longitude-0.0) < 0.01) OR (Longitude < -91.6)")
dbClearResult(res)
res <- dbSendQuery(con, "
   UPDATE crime SET [X.Coordinate]=NULL
   WHERE ([X.Coordinate]='') OR ([X.Coordinate]=0)")
dbClearResult(res)
res <- dbSendQuery(con, "
   UPDATE crime SET [Y.Coordinate]=NULL
   WHERE ([Y.Coordinate]='') OR ([Y.Coordinate]=0)")
dbClearResult(res)
```

Let's rerun that query and check that we get more sensible results.
```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
   SELECT MIN(Latitude)  AS min_lat,
          MAX(Latitude)  AS max_lat,
          MIN(Longitude) AS min_lon,
          MAX(Longitude) AS max_lon,
          District
   FROM crime
   GROUP BY District
   ORDER BY District")
fetch(res, n = -1)
dbClearResult(res)
```
Now we have results that are more in line with where Chicago actually is. Make it a habit to do some checks of your data before doing too much analysis.

And what city does the following plot have the shape of?
Let's plot the location of these crimes. Plotting all 7 million would be overkill, so let's take a random sample of 10,000 crimes. Here's a SQL query that will do that. It uses some tricks we'll learn more about later including the use of `IN`, the use of subqueries (a query within a query), and `LIMIT`. Does the shape of the plot look right?
```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
   SELECT Longitude, Latitude 
   FROM crime 
   WHERE id IN (SELECT id FROM crime ORDER BY RANDOM() LIMIT 10000)")
a <- fetch(res, n = -1)
dbClearResult(res)

plot(Latitude~Longitude, data=a, pch=".", xlab="Longitude", ylab="Latitude")
```

# Practice exercises

`r .exNum('Plot the location of all "ASSAULT"s for Ward 22')`


`r .exNum('What is the most common (Lat,Long) for assaults in Ward 22? Add the point to your plot using the @@points()@@ function. @@points()@@ simply draws a point (or sequence of points) at the specified coordinates')`


# Solutions to the exercises

`r .exQ(1)`

```{r comment="", results='hold'}
res <- dbSendQuery(con, "
          SELECT *
          FROM crime
          WHERE ((Beat=234))") 
a <- fetch(res, n = -1) 
dbClearResult(res)
```


`r .exQ(2)`

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                  SELECT Beat, District, Ward, [Community.Area], [Primary.Type]
                  FROM crime
                  WHERE (([Primary.Type]='ASSAULT'))") 
a <- fetch(res, n = -1) 
dbClearResult(res)
```

`r .exQ(3)`

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                    SELECT *
                    FROM crime
                    WHERE ((Beat=234) AND ([Primary.Type]='ASSAULT'))")
a <- fetch(res, n = -1) 
dbClearResult(res)
```

`r .exQ(4)`

```{r comment="", results='hold'} 
# system.time() reports how long it takes to run the SQL query
system.time(
{
   res <- dbSendQuery(con, "
                      SELECT *
                      FROM crime
                      WHERE ((IUCR='0560') AND ([Primary.Type]='ASSAULT'))") 
   a <- fetch(res, n = -1) 
})
dbClearResult(res)
table(a$Ward)
```
Or, we could also try selecting all the IUCR codes and ward and then subsetting the data through R.
```{r comment="", results='hold'} 
system.time(
{
   res <- dbSendQuery(con, "
                      SELECT IUCR, Ward, [Primary.Type]
                      FROM crime")
   data <- fetch(res, n = -1)
   data <- subset(data, Primary.Type=="ASSAULT" & IUCR=="0560")
})
dbClearResult(res)
```

`r .exQ(5)`

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
              SELECT COUNT(*) AS count, [Primary.Type]
              FROM crime
              GROUP BY [Primary.Type]")
fetch(res, n = -1)
dbClearResult(res)
```

`r .exQ(6)`

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                   SELECT COUNT(*) AS count, [Primary.Type]
                   FROM crime
                   WHERE Arrest='true'
                   GROUP BY [Primary.Type]")
fetch(res, n = -1)
dbClearResult(res)
```

Or, if we weren't interested in differentiating based on the `Primary.Type`, we could simply do the following:

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                   SELECT COUNT(*) AS count
                   FROM crime
                   WHERE Arrest='true'")
fetch(res, n = -1)
dbClearResult(res)
```


`r .exQ(7)`

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
SELECT COUNT(*) AS count,
                   [Location.Description]
                   FROM crime
                   GROUP BY [Location.Description]")
fetch(res, n = -1)
dbClearResult(res)
```

`r .exQ(8)`

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
                   SELECT Latitude, Longitude
                   FROM crime
                   WHERE [Primary.Type]='ASSAULT' AND Ward='22'")
a <- fetch(res, n = -1)
dbClearResult(res)
plot(Latitude~Longitude, data=a, pch=".")
```


`r .exQ(9)`

```{r comment="", results='hold'} 
res <- dbSendQuery(con, "
        SELECT COUNT(*) as crimecount,
               Longitude, Latitude
        FROM crime
        WHERE [Primary.Type]='ASSAULT' and Ward='22'
        GROUP BY Longitude, Latitude")
b <- fetch(res, n=-1)
dbClearResult(res)

plot(Latitude~Longitude, data=a, pch=".")
points(b[which.max(b$crimecount), 2:3],
       pch=16, 
       col="salmon",
       cex=4)
b[which.max(b$crimecount), 2:3]
```
